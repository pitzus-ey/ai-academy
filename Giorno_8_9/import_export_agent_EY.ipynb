{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d629e8",
   "metadata": {},
   "source": [
    "# Funzione per il calcolo degli Embeddings\n",
    "\n",
    "Di seguito implementiamo una funzione per calcolare gli embeddings utilizzando Azure OpenAI. Questa funzione accetta in input un testo (o una lista di testi) e restituisce i corrispondenti embeddings utilizzando il modello specificato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(text_input, client=None, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Calcola gli embeddings per un testo o una lista di testi\n",
    "    \n",
    "    Args:\n",
    "        text_input: Testo o lista di testi di cui calcolare l'embedding\n",
    "        client: Client Azure OpenAI (se None, viene usato il client di default)\n",
    "        model: Nome del modello di embedding da utilizzare\n",
    "        \n",
    "    Returns:\n",
    "        Lista di embeddings o singolo embedding a seconda dell'input\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Usa il client di default se non specificato\n",
    "    if client is None:\n",
    "        # Assicurati che il client sia definito globalmente\n",
    "        try:\n",
    "            global client\n",
    "        except NameError:\n",
    "            raise ValueError(\"Client OpenAI non definito. Inizializza prima il client.\")\n",
    "    \n",
    "    # Normalizza input - assicurandoci che sia sempre una lista\n",
    "    is_single_input = False\n",
    "    if isinstance(text_input, str):\n",
    "        text_input = [text_input]\n",
    "        is_single_input = True\n",
    "    \n",
    "    # Pulisci i testi\n",
    "    import re\n",
    "    cleaned_texts = [re.sub(r'\\s+', ' ', text.strip()) for text in text_input]\n",
    "    \n",
    "    try:\n",
    "        # Chiama l'API per ottenere gli embeddings\n",
    "        response = client.embeddings.create(\n",
    "            input=cleaned_texts,\n",
    "            model=model\n",
    "        )\n",
    "        \n",
    "        # Estrai gli embeddings dalla risposta\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        \n",
    "        # Restituisci un singolo embedding o una lista basandosi sull'input originale\n",
    "        if is_single_input:\n",
    "            return embeddings[0]\n",
    "        else:\n",
    "            return embeddings\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il calcolo degli embeddings: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test della funzione compute_embeddings\n",
    "try:\n",
    "    # Test con un singolo testo\n",
    "    test_text = \"Questo è un testo di prova per testare la funzione di embedding.\"\n",
    "    single_embedding = compute_embeddings(test_text)\n",
    "    print(f\"Dimensione dell'embedding per un singolo testo: {len(single_embedding)}\")\n",
    "    \n",
    "    # Test con una lista di testi\n",
    "    test_texts = [\n",
    "        \"Primo testo di esempio\",\n",
    "        \"Secondo testo di esempio per embedding\"\n",
    "    ]\n",
    "    multiple_embeddings = compute_embeddings(test_texts)\n",
    "    print(f\"Numero di embeddings calcolati: {len(multiple_embeddings)}\")\n",
    "    print(f\"Dimensione di ciascun embedding: {len(multiple_embeddings[0])}\")\n",
    "    \n",
    "    # Calcola similarità tra i due testi\n",
    "    import numpy as np\n",
    "    def cosine_similarity(a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    similarity = cosine_similarity(multiple_embeddings[0], multiple_embeddings[1])\n",
    "    print(f\"Similarità coseno tra i due testi: {similarity:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Errore durante il test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d717e",
   "metadata": {},
   "source": [
    "# Utilizzo della funzione di embedding nel Pipeline RAG\n",
    "\n",
    "Vediamo come integrare la funzione di embedding nel nostro sistema RAG. Useremo questa funzione per:\n",
    "\n",
    "1. Generare embeddings per il documento di input (query)\n",
    "2. Confrontare questi embeddings con quelli dei nostri chunks per trovare le informazioni più rilevanti\n",
    "3. Utilizzare le informazioni più rilevanti per arricchire il prompt per il modello linguistico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio pratico di utilizzo degli embeddings nel sistema RAG\n",
    "\n",
    "def rag_pipeline(document_text, chunks_data, top_k=3):\n",
    "    \"\"\"\n",
    "    Pipeline RAG completa: prende un documento, trova chunks rilevanti e genera una risposta\n",
    "    \n",
    "    Args:\n",
    "        document_text: Il testo del documento da analizzare\n",
    "        chunks_data: Lista di chunks con embeddings\n",
    "        top_k: Numero di chunks più rilevanti da utilizzare\n",
    "    \n",
    "    Returns:\n",
    "        Risposta generata dal modello\n",
    "    \"\"\"\n",
    "    # 1. Calcola embedding per il documento (usando solo i primi 1000 caratteri come query)\n",
    "    print(\"Generazione embedding per il documento di input...\")\n",
    "    query_text = document_text[:1000]  # Usa i primi 1000 caratteri come query\n",
    "    query_embedding = compute_embeddings(query_text)\n",
    "    \n",
    "    # 2. Trova i chunks più rilevanti in base alla similarità\n",
    "    print(\"Ricerca chunks rilevanti...\")\n",
    "    relevant_chunks = get_relevant_chunks(query_embedding, chunks_data, top_k=top_k)\n",
    "    \n",
    "    # 3. Estrai il contenuto dei chunk rilevanti\n",
    "    context = \"\\n\\n\".join([chunk['content'] for chunk in relevant_chunks])\n",
    "    print(f\"Trovati {len(relevant_chunks)} chunks rilevanti\")\n",
    "    \n",
    "    # 4. Crea il prompt RAG-enhanced\n",
    "    prompt = RAG_ENHANCED_PROMPT.format(context=context, documento=document_text)\n",
    "    \n",
    "    # 5. Chiama l'API con il prompt arricchito\n",
    "    print(\"Generazione risposta con Azure OpenAI...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Sei un analizzatore testi professionista. In base alle mail che trovi nel testo che ti passo, fornisci come prima risposta un riepilogo sulle richieste dei clienti. E poi formula una risposta per ogni cliente sulla base della sua richiesta.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_completion_tokens=1024,\n",
    "        temperature=1,\n",
    "    )\n",
    "    \n",
    "    # 6. Restituisci la risposta\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Esempio di utilizzo\n",
    "# Per testare con un singolo documento:\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Carica un documento di esempio\n",
    "file = \"email1.txt\"  # Scegli uno dei file nella cartella documents\n",
    "with open(os.path.join(\"documents\", file), \"r\", encoding=\"utf-8\") as f:\n",
    "    document_text = f.read()\n",
    "\n",
    "# Esegui il pipeline RAG\n",
    "response = rag_pipeline(document_text, chunks_data, top_k=3)\n",
    "print(\"\\nRisposta dalla pipeline RAG:\")\n",
    "print(response)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d663438",
   "metadata": {},
   "source": [
    "# Nota sull'utilizzo della funzione di embedding\n",
    "\n",
    "La funzione `compute_embeddings` può essere utilizzata per sostituire le chiamate dirette al client Azure OpenAI. Per esempio, nel loop di elaborazione dei documenti, invece di scrivere:\n",
    "\n",
    "```python\n",
    "query_response = client.embeddings.create(\n",
    "    input=document_text[:1000],\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "query_embedding = query_response.data[0].embedding\n",
    "```\n",
    "\n",
    "È possibile utilizzare semplicemente:\n",
    "\n",
    "```python\n",
    "query_embedding = compute_embeddings(document_text[:1000])\n",
    "```\n",
    "\n",
    "Questo approccio ha diversi vantaggi:\n",
    "1. Codice più pulito e leggibile\n",
    "2. Gestione centralizzata degli errori\n",
    "3. Facilità di modificare i parametri o il comportamento della funzione in un solo punto\n",
    "4. Supporto per input singoli o multipli con la stessa interfaccia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa6082",
   "metadata": {},
   "source": [
    "## Pipeline NER e anonimizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2a9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[email1.txt] → Tipo rilevato: Mail\n",
      "[email10.txt] → Tipo rilevato: Mail\n",
      "[email2.txt] → Tipo rilevato: Mail\n",
      "[email3.txt] → Tipo rilevato: Mail\n",
      "[email4.txt] → Tipo rilevato: Mail\n",
      "[email5.txt] → Tipo rilevato: Mail\n",
      "[email6.txt] → Tipo rilevato: Mail\n",
      "[email7.txt] → Tipo rilevato: Mail\n",
      "[email8.txt] → Tipo rilevato: Mail\n",
      "[email9.txt] → Tipo rilevato: Mail\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "import os\n",
    "import certifi\n",
    "from huggingface_hub import hf_hub_download, HfApi\n",
    "\n",
    "# Forza HuggingFace Hub a usare il certificato custom (Zscaler incluso)\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "os.environ['CURL_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", \n",
    "                   model=\"Davlan/bert-base-multilingual-cased-ner-hrl\",\n",
    "                   aggregation_strategy=\"simple\"\n",
    "                   )\n",
    "# Path to your local model folder\n",
    "#model_path = r\"C:\\desktopnodrive\\ai-academy\\xlm-roberta-base-ner-hrl\"\n",
    " \n",
    "# Load tokenizer and model\n",
    "#tokenizer =AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "#model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    " \n",
    "# Create the pipeline\n",
    "#ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")       \n",
    " \n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    " \n",
    "# Chat completion model credentials\n",
    "azure_chat_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_chat_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_chat_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "azure_chat_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    " \n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=azure_chat_api_key,\n",
    "    azure_endpoint=azure_chat_endpoint,\n",
    "    api_version=azure_chat_api_version,\n",
    "    )\n",
    "DOCUMENT_TYPE_PROMPT = \"\"\"\n",
    "Ti fornirò un documento aziendale.\n",
    " \n",
    "Il tuo compito è determinare il tipo di documento, scegliendo tra i seguenti: \n",
    "- Mail\n",
    "- Nota di credito\n",
    "- Ordine di acquisto\n",
    "- Contratto\n",
    "- Altro\n",
    " \n",
    "Devi basarti solo sul contenuto del documento fornito.\n",
    " \n",
    "Ora incollerò il contenuto del documento tra tripli apici. Rispondi semplicemente con il tipo, nulla di più.\n",
    " \n",
    "Documento:\n",
    "'''\n",
    "{documento}\n",
    "'''\n",
    "\"\"\"\n",
    " \n",
    " \n",
    "def anonymize_documents():\n",
    "    os.makedirs(\"anonymized\", exist_ok=True)\n",
    "    for file in os.listdir(\"documents\"):\n",
    "        with open(os.path.join(\"documents\", file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        iban_pattern = r'\\b[A-Z]{2}\\d{2}[A-Z0-9]{1,30}\\b'\n",
    "        iban_pattern1 = r'\\b[A-Z]{2}\\d{2}(?:\\s?[A-Z0-9]{1,4}){1,7}\\b'\n",
    "        fiscal_code_pattern = r'\\b([A-Z]{6}\\d{2}[A-Z]\\d{2}[A-Z]\\d{3}[A-Z])\\b'\n",
    "        cell_number_pattern = r'\\b(?:\\+39[\\s\\.]?)?3[\\d\\s\\.]{8,12}\\b' \n",
    "        landline_pattern = r'\\b0\\d{1,3}([\\s\\.]?\\d{2,4}){1,3}\\b'# Italian cell number pattern (e.g., 3XXYYYYYYY or +393XXXXXXXXX)\n",
    "        email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
    "        # Get NER results with simple aggregation strategy\n",
    "        model_entities = ner_pipeline(text)\n",
    "        # Manually find start and end positions for model results\n",
    "        for entity in model_entities:\n",
    "            if 'start' not in entity or entity['start'] is None:\n",
    "                # Find the entity word in the original text\n",
    "                entity_text = entity['word']\n",
    "                # Remove leading ▁ if present (XLM-RoBERTa tokenizer specific)\n",
    "                clean_text = entity_text.replace('▁', ' ').strip()\n",
    "                # Find position in original text\n",
    "                position = text.find(clean_text)\n",
    "                if position != -1:\n",
    "                    entity['start'] = position\n",
    "                    entity['end'] = position + len(clean_text)\n",
    "                    # Update word to match the text in the document\n",
    "                    entity['word'] = clean_text\n",
    "        # Convert model entities to the same format as regex matches\n",
    "        results = []\n",
    "        for entity in model_entities:\n",
    "            if 'start' in entity and entity['start'] is not None:\n",
    "                results.append({\n",
    "                    \"entity\": f\"B-{entity['entity_group']}\",\n",
    "                    \"score\": entity['score'],\n",
    "                    \"index\": -1,\n",
    "                    \"word\": entity['word'],\n",
    "                    \"start\": entity['start'],\n",
    "                    \"end\": entity['end']\n",
    "                })\n",
    "        # Add IBAN matches as NER-like dicts\n",
    "        iban_matches = []\n",
    "        for match in re.finditer(iban_pattern, text):\n",
    "            iban_matches.append({\n",
    "                \"entity\": \"B-IBAN\",\n",
    "                \"score\": 1.0,\n",
    "                \"index\": -1,\n",
    "                \"word\": match.group(),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        # Add IBAN matches as NER-like dicts\n",
    "        iban_matches1 = []\n",
    "        for match in re.finditer(iban_pattern1, text):\n",
    "            iban_matches1.append({\n",
    "                \"entity\": \"B-IBAN\",\n",
    "                \"score\": 1.0,\n",
    "                \"index\": -1,\n",
    "                \"word\": match.group(),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        # Add Fiscal Code matches as NER-like dicts\n",
    "        fiscal_code_matches = []\n",
    "        for match in re.finditer(fiscal_code_pattern, text):\n",
    "            fiscal_code_matches.append({\n",
    "                \"entity\": \"B-FISCALCODE\",\n",
    "                \"score\": 1.0,\n",
    "                \"index\": -1,\n",
    "                \"word\": match.group(),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        # Add Cell Number matches as NER-like dicts\n",
    "        cell_number_matches = []\n",
    "        for match in re.finditer(cell_number_pattern, text):\n",
    "            cell_number_matches.append({\n",
    "                \"entity\": \"B-CELLNUMBER\",\n",
    "                \"score\": 1.0,\n",
    "                \"index\": -1,\n",
    "                \"word\": match.group(),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        # Add Cell Number matches as NER-like dicts\n",
    "        phone_number_matches = []\n",
    "        for match in re.finditer(landline_pattern, text):\n",
    "            phone_number_matches.append({\n",
    "                \"entity\": \"B-CELLNUMBER\",\n",
    "                \"score\": 1.0,\n",
    "                \"index\": -1,\n",
    "                \"word\": match.group(),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        # Add Email Address matches as NER-like dicts\n",
    "        email_matches = []\n",
    "        for match in re.finditer(email_pattern, text):\n",
    "            email_matches.append({\n",
    "                \"entity\": \"B-EMAIL\",\n",
    "                \"score\": 1.0,\n",
    "                \"index\": -1,\n",
    "                \"word\": match.group(),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        # Insert regex matches at the beginning\n",
    "        results = iban_matches + iban_matches1 + fiscal_code_matches + cell_number_matches + phone_number_matches + email_matches + results\n",
    "        # Reconstruct entities\n",
    "        entities = []\n",
    "        current = None\n",
    "        for ent in results:\n",
    "            if ent[\"entity\"].startswith(\"B-\"):\n",
    "                if current:\n",
    "                    entities.append(current)\n",
    "                current = {\n",
    "                    \"entity\": ent[\"entity\"][2:],\n",
    "                    \"tokens\": [ent[\"word\"]],\n",
    "                    \"start\": ent[\"start\"],\n",
    "                    \"end\": ent[\"end\"]\n",
    "                }\n",
    "            elif ent[\"entity\"].startswith(\"I-\") and current:\n",
    "                current[\"tokens\"].append(ent[\"word\"])\n",
    "                current[\"end\"] = ent[\"end\"]\n",
    "        if current:\n",
    "            entities.append(current)\n",
    "        # Map for anonymization\n",
    "        label_map = {\n",
    "            \"EMAIL\": \"Email\",\n",
    "            \"PER\": \"Nome\",\n",
    "            \"ORG\": \"Azienda\",\n",
    "            \"LOC\": \"Luogo\",\n",
    "            \"IBAN\": \"IBAN\",\n",
    "            \"FISCALCODE\": \"FISCALCODE\",\n",
    "            \"CELLNUMBER\": \"Cellulare\",\n",
    "        }\n",
    "        entity_counters = {}\n",
    "        replacements = []\n",
    "        for ent in entities:\n",
    "            label = ent[\"entity\"]\n",
    "            mapped = label_map.get(label, label)\n",
    "            entity_counters[mapped] = entity_counters.get(mapped, 0) + 1\n",
    " \n",
    "            # Custom anonymization for IBAN and FISCALCODE\n",
    "            entity_text = \" \".join(ent[\"tokens\"]).strip()\n",
    "            if label == \"IBAN\":\n",
    "                # Mask all but first 4 chars\n",
    "                masked = entity_text[:4] + \"*\" * (len(entity_text) - 4)\n",
    "                placeholder = f\"[{masked}]\"\n",
    "            elif label == \"FISCALCODE\":\n",
    "                # Mask all but first 3 chars\n",
    "                masked = entity_text[:3] + \"*\" * (len(entity_text) - 3) \n",
    "                placeholder = f\"[{masked}]\"\n",
    "            elif label == \"CELLNUMBER\":\n",
    "                masked = entity_text[:3] + \"*\" * (len(entity_text) - 3)\n",
    "                placeholder = f\"[{masked}]\"\n",
    "            else:\n",
    "                placeholder = f\"[{mapped}_{entity_counters[mapped]}]\"\n",
    " \n",
    "            replacements.append((entity_text, placeholder, ent))\n",
    "        # Sort replacements by start position in reverse order (to avoid position shifts)\n",
    "        replacements.sort(key=lambda x: x[2][\"start\"], reverse=True)\n",
    "        # Replace entities in text\n",
    "        anonymized_chars = list(text)\n",
    "        for entity_text, placeholder, entity in replacements:\n",
    "            start = entity[\"start\"]\n",
    "            end = entity[\"end\"]\n",
    "            # Replace the characters at the specified positions\n",
    "            anonymized_chars[start:end] = placeholder\n",
    "        anonymized_text = ''.join(anonymized_chars)\n",
    "        # Write anonymized file\n",
    "        with open(os.path.join(\"anonymized\", file), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(anonymized_text)\n",
    " \n",
    "def get_chat_response(prompt):\n",
    " \n",
    "    response = client.chat.completions.create(\n",
    "    model=azure_chat_deployment,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Sei un assistente AI.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_completion_tokens=256,\n",
    "    temperature=1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    " \n",
    "def test_documents():\n",
    "    anonymize_documents()\n",
    "    for file in os.listdir(\"anonymized\"):\n",
    "        with open(os.path.join(\"anonymized\", file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        prompt = DOCUMENT_TYPE_PROMPT.format(documento=text)\n",
    "        response = get_chat_response(prompt)\n",
    "        print(f\"[{file}] → Tipo rilevato: {response}\")\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    test_documents()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666c27b",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento variabili d'ambiente...\n",
      "ADA_ENDPOINT: https://fedocana.openai.azure.com/\n",
      "ADA_API_KEY: ************************************************************************************\n",
      "Processando PDF: C:\\Users\\XM745EF\\OneDrive - EY\\Documents\\AI Academy\\Hackaton-agentic-RAG\\Gruppo_8\\pdf_documents\\Wiki_Conformità_primi_50.pdf\n",
      "Estratto testo da 50 pagine con PyMuPDF\n",
      "Creati 201 chunks dal documento\n",
      "Creazione embeddings in corso...\n",
      "Processati 10/201 chunks (5.0%)\n",
      "Processati 20/201 chunks (10.0%)\n",
      "Processati 30/201 chunks (14.9%)\n",
      "Processati 40/201 chunks (19.9%)\n",
      "Processati 50/201 chunks (24.9%)\n",
      "Processati 60/201 chunks (29.9%)\n",
      "Processati 70/201 chunks (34.8%)\n",
      "Processati 80/201 chunks (39.8%)\n",
      "Processati 90/201 chunks (44.8%)\n",
      "Processati 100/201 chunks (49.8%)\n",
      "Processati 110/201 chunks (54.7%)\n",
      "Processati 120/201 chunks (59.7%)\n",
      "Processati 130/201 chunks (64.7%)\n",
      "Processati 140/201 chunks (69.7%)\n",
      "Processati 150/201 chunks (74.6%)\n",
      "Processati 160/201 chunks (79.6%)\n",
      "Processati 170/201 chunks (84.6%)\n",
      "Processati 180/201 chunks (89.6%)\n",
      "Processati 190/201 chunks (94.5%)\n",
      "Processati 200/201 chunks (99.5%)\n",
      "Processati 201/201 chunks (100.0%)\n",
      "Completati embeddings per 201/201 chunks\n",
      "Chunks salvati in:\n",
      "  - chunks_with_embeddings.pkl (con embeddings)\n",
      "  - chunks_with_embeddings_metadata.json (metadata)\n",
      "\n",
      "=== STATISTICHE FINALI ===\n",
      "Chunks totali: 201\n",
      "Pagine processate: 50\n",
      "Dimensione media chunk: 1233 caratteri\n",
      "Chunks con embeddings: 201\n",
      "Sezioni identificate: ['directive', 'decree', 'general', 'article', 'definition', 'requirements', 'procedure']\n",
      "Tipi documento: ['regolamento', 'direttiva', 'decreto', 'normativa']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "# Gestione import PyMuPDF con conflitti\n",
    "try:\n",
    "    import pymupdf as fitz\n",
    "except ImportError:\n",
    "    try:\n",
    "        import fitz\n",
    "    except ImportError:\n",
    "        # Fallback usando PyPDF2 se PyMuPDF non funziona\n",
    "        try:\n",
    "            import PyPDF2\n",
    "            PYMUPDF_AVAILABLE = False\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Nessuna libreria PDF disponibile. Installa PyMuPDF o PyPDF2\")\n",
    "    else:\n",
    "        PYMUPDF_AVAILABLE = True\n",
    "else:\n",
    "    PYMUPDF_AVAILABLE = True\n",
    "\n",
    "# Carica le variabili d'ambiente\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"Rappresenta un chunk del documento con i suoi metadati\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    page_number: int\n",
    "    chunk_index: int\n",
    "    section: str\n",
    "    subsection: str\n",
    "    document_type: str\n",
    "    embedding: List[float] = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "class PDFEmbeddingProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Inizializza il processore con il client Azure OpenAI\"\"\"\n",
    "        # Debug: stampa le variabili caricate\n",
    "        print(\"Caricamento variabili d'ambiente...\")\n",
    "        \n",
    "        self.endpoint = os.getenv(\"ADA_ENDPOINT\")\n",
    "        print(f\"ADA_ENDPOINT: {self.endpoint}\")\n",
    "        if not self.endpoint:\n",
    "            raise ValueError(\"Devi definire ADA_ENDPOINT nel file .env\")\n",
    "        \n",
    "        self.api_key = os.getenv(\"ADA_API_KEY\")\n",
    "        print(f\"ADA_API_KEY: {'*' * len(self.api_key) if self.api_key else 'None'}\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Devi definire ADA_API_KEY nel file .env\")\n",
    "        \n",
    "        self.api_version = os.getenv(\"ADA_API_VERSION\", \"2024-02-01\")\n",
    "        self.deployment_name = os.getenv(\"ADA_DEPLOYMENT_NAME\", \"text-embedding-ada-002\")\n",
    "        \n",
    "        # Inizializza il client Azure OpenAI\n",
    "        self.client = AzureOpenAI(\n",
    "            azure_endpoint=self.endpoint,\n",
    "            api_key=self.api_key,\n",
    "            api_version=self.api_version\n",
    "        )\n",
    "        \n",
    "        # Configurazione chunking\n",
    "        self.max_chunk_size = 1500  # Caratteri per chunk\n",
    "        self.overlap_size = 200     # Overlap tra chunks\n",
    "        \n",
    "        # Pattern per identificare sezioni nel documento normativo\n",
    "        self.section_patterns = {\n",
    "            'regulation': r'Reg\\.to \\(.*?\\)',\n",
    "            'directive': r'Direttiva \\d+/\\d+/[A-Z]+',\n",
    "            'article': r'art\\.\\s*\\d+|articolo\\s*\\d+',\n",
    "            'decree': r'Decreto.*?\\d{4}',\n",
    "            'chapter': r'Capo\\s+[IVX]+|Titolo\\s+[IVX]+',\n",
    "            'definition': r'Definizioni:|definizioni:',\n",
    "            'requirements': r'Requisiti|Obblighi',\n",
    "            'procedure': r'Procedur[ae]|Modalità'\n",
    "        }\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Estrae il testo dal PDF mantenendo informazioni sulla pagina\"\"\"\n",
    "        pages_content = []\n",
    "        \n",
    "        if PYMUPDF_AVAILABLE:\n",
    "            return self._extract_with_pymupdf(pdf_path)\n",
    "        else:\n",
    "            return self._extract_with_pypdf2(pdf_path)\n",
    "    \n",
    "    def _extract_with_pymupdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Estrae testo usando PyMuPDF\"\"\"\n",
    "        pages_content = []\n",
    "        \n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            \n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                text = page.get_text()\n",
    "                \n",
    "                if text.strip():  # Solo pagine con contenuto\n",
    "                    pages_content.append({\n",
    "                        'page_number': page_num + 1,\n",
    "                        'content': text,\n",
    "                        'char_count': len(text)\n",
    "                    })\n",
    "            \n",
    "            doc.close()\n",
    "            print(f\"Estratto testo da {len(pages_content)} pagine con PyMuPDF\")\n",
    "            return pages_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'estrazione del PDF con PyMuPDF: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _extract_with_pypdf2(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Estrae testo usando PyPDF2 come fallback\"\"\"\n",
    "        pages_content = []\n",
    "        \n",
    "        try:\n",
    "            import PyPDF2\n",
    "            \n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                \n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    text = page.extract_text()\n",
    "                    \n",
    "                    if text.strip():  # Solo pagine con contenuto\n",
    "                        pages_content.append({\n",
    "                            'page_number': page_num + 1,\n",
    "                            'content': text,\n",
    "                            'char_count': len(text)\n",
    "                        })\n",
    "            \n",
    "            print(f\"Estratto testo da {len(pages_content)} pagine con PyPDF2\")\n",
    "            return pages_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'estrazione del PDF con PyPDF2: {e}\")\n",
    "            return []\n",
    "\n",
    "    def identify_section(self, text: str) -> tuple:\n",
    "        \"\"\"Identifica la sezione e sottosezione del testo\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Identifica il tipo di sezione principale\n",
    "        section = \"general\"\n",
    "        for section_type, pattern in self.section_patterns.items():\n",
    "            if re.search(pattern, text, re.IGNORECASE):\n",
    "                section = section_type\n",
    "                break\n",
    "        \n",
    "        # Estrae sottosezione specifica\n",
    "        subsection = \"\"\n",
    "        \n",
    "        # Per regolamenti e direttive\n",
    "        reg_match = re.search(r'(Reg\\.to.*?\\d{4}/\\d+|Direttiva.*?\\d{4}/\\d+)', text, re.IGNORECASE)\n",
    "        if reg_match:\n",
    "            subsection = reg_match.group(1)\n",
    "        \n",
    "        # Per articoli\n",
    "        art_match = re.search(r'(art\\.\\s*\\d+[a-z]*|articolo\\s*\\d+[a-z]*)', text, re.IGNORECASE)\n",
    "        if art_match:\n",
    "            subsection = art_match.group(1)\n",
    "        \n",
    "        # Per definizioni specifiche\n",
    "        if 'definizioni' in text_lower:\n",
    "            def_match = re.search(r'([a-zA-Z\\s]+):', text)\n",
    "            if def_match:\n",
    "                subsection = f\"def_{def_match.group(1).strip()}\"\n",
    "        \n",
    "        return section, subsection\n",
    "\n",
    "    def smart_chunk_text(self, text: str, page_number: int) -> List[str]:\n",
    "        \"\"\"Divide il testo in chunks intelligenti basati sulla struttura del documento\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Dividi per paragrafi principali\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            paragraph = paragraph.strip()\n",
    "            if not paragraph:\n",
    "                continue\n",
    "            \n",
    "            # Se il paragrafo da solo è troppo lungo, dividilo\n",
    "            if len(paragraph) > self.max_chunk_size:\n",
    "                # Salva il chunk corrente se non vuoto\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "                \n",
    "                # Dividi il paragrafo lungo per frasi\n",
    "                sentences = re.split(r'[.!?]+', paragraph)\n",
    "                temp_chunk = \"\"\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if not sentence:\n",
    "                        continue\n",
    "                    \n",
    "                    if len(temp_chunk + sentence) > self.max_chunk_size:\n",
    "                        if temp_chunk:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = sentence\n",
    "                    else:\n",
    "                        temp_chunk += sentence + \". \"\n",
    "                \n",
    "                if temp_chunk:\n",
    "                    current_chunk = temp_chunk\n",
    "            \n",
    "            # Se aggiungere questo paragrafo supera la dimensione massima\n",
    "            elif len(current_chunk + paragraph) > self.max_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "        \n",
    "        # Aggiungi l'ultimo chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def create_chunks(self, pages_content: List[Dict[str, Any]]) -> List[DocumentChunk]:\n",
    "        \"\"\"Crea chunks dal contenuto delle pagine\"\"\"\n",
    "        all_chunks = []\n",
    "        chunk_id_counter = 0\n",
    "        \n",
    "        for page_data in pages_content:\n",
    "            page_number = page_data['page_number']\n",
    "            content = page_data['content']\n",
    "            \n",
    "            # Dividi il contenuto della pagina in chunks\n",
    "            text_chunks = self.smart_chunk_text(content, page_number)\n",
    "            \n",
    "            for chunk_index, chunk_text in enumerate(text_chunks):\n",
    "                if len(chunk_text.strip()) < 50:  # Salta chunks troppo piccoli\n",
    "                    continue\n",
    "                \n",
    "                # Identifica sezione e sottosezione\n",
    "                section, subsection = self.identify_section(chunk_text)\n",
    "                \n",
    "                # Determina il tipo di documento\n",
    "                doc_type = \"normativa\"\n",
    "                if any(keyword in chunk_text.lower() for keyword in ['regolamento', 'reg.to']):\n",
    "                    doc_type = \"regolamento\"\n",
    "                elif any(keyword in chunk_text.lower() for keyword in ['direttiva']):\n",
    "                    doc_type = \"direttiva\"\n",
    "                elif any(keyword in chunk_text.lower() for keyword in ['decreto']):\n",
    "                    doc_type = \"decreto\"\n",
    "                \n",
    "                # Crea metadata\n",
    "                metadata = {\n",
    "                    'char_count': len(chunk_text),\n",
    "                    'word_count': len(chunk_text.split()),\n",
    "                    'contains_definitions': 'definizioni' in chunk_text.lower(),\n",
    "                    'contains_requirements': any(req in chunk_text.lower() for req in ['requisiti', 'obblighi', 'deve', 'devono']),\n",
    "                    'contains_procedures': any(proc in chunk_text.lower() for proc in ['procedura', 'modalità', 'metodo']),\n",
    "                    'extraction_timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                chunk = DocumentChunk(\n",
    "                    id=f\"chunk_{chunk_id_counter:06d}\",\n",
    "                    content=chunk_text,\n",
    "                    page_number=page_number,\n",
    "                    chunk_index=chunk_index,\n",
    "                    section=section,\n",
    "                    subsection=subsection,\n",
    "                    document_type=doc_type,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                \n",
    "                all_chunks.append(chunk)\n",
    "                chunk_id_counter += 1\n",
    "        \n",
    "        print(f\"Creati {len(all_chunks)} chunks dal documento\")\n",
    "        return all_chunks\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Ottiene l'embedding per il testo usando Azure OpenAI\"\"\"\n",
    "        try:\n",
    "            # Pulisce il testo\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', text.strip())\n",
    "            \n",
    "            response = self.client.embeddings.create(\n",
    "                input=cleaned_text,\n",
    "                model=self.deployment_name\n",
    "            )\n",
    "            \n",
    "            return response.data[0].embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel creare embedding: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_embeddings(self, chunks: List[DocumentChunk], batch_size: int = 10) -> List[DocumentChunk]:\n",
    "        \"\"\"Crea embeddings per tutti i chunks\"\"\"\n",
    "        print(\"Creazione embeddings in corso...\")\n",
    "        \n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "            \n",
    "            for chunk in batch:\n",
    "                embedding = self.get_embedding(chunk.content)\n",
    "                if embedding:\n",
    "                    chunk.embedding = embedding\n",
    "                else:\n",
    "                    print(f\"Fallito embedding per chunk {chunk.id}\")\n",
    "            \n",
    "            # Progress update\n",
    "            progress = min(i + batch_size, len(chunks))\n",
    "            print(f\"Processati {progress}/{len(chunks)} chunks ({progress/len(chunks)*100:.1f}%)\")\n",
    "        \n",
    "        # Filtra chunks senza embedding\n",
    "        chunks_with_embeddings = [chunk for chunk in chunks if chunk.embedding is not None]\n",
    "        print(f\"Completati embeddings per {len(chunks_with_embeddings)}/{len(chunks)} chunks\")\n",
    "        \n",
    "        return chunks_with_embeddings\n",
    "\n",
    "    def save_chunks(self, chunks: List[DocumentChunk], output_path: str):\n",
    "        \"\"\"Salva i chunks con embeddings\"\"\"\n",
    "        # Converti in formato serializzabile\n",
    "        chunks_data = []\n",
    "        for chunk in chunks:\n",
    "            chunk_dict = {\n",
    "                'id': chunk.id,\n",
    "                'content': chunk.content,\n",
    "                'page_number': chunk.page_number,\n",
    "                'chunk_index': chunk.chunk_index,\n",
    "                'section': chunk.section,\n",
    "                'subsection': chunk.subsection,\n",
    "                'document_type': chunk.document_type,\n",
    "                'embedding': chunk.embedding,\n",
    "                'metadata': chunk.metadata\n",
    "            }\n",
    "            chunks_data.append(chunk_dict)\n",
    "        \n",
    "        # Salva in formato pickle per mantenere gli embeddings\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(chunks_data, f)\n",
    "        \n",
    "        # Salva anche un file JSON senza embeddings per ispezione\n",
    "        json_path = output_path.replace('.pkl', '_metadata.json')\n",
    "        json_data = []\n",
    "        for chunk_dict in chunks_data:\n",
    "            json_chunk = chunk_dict.copy()\n",
    "            del json_chunk['embedding']  # Rimuovi embedding per JSON\n",
    "            json_data.append(json_chunk)\n",
    "        \n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Chunks salvati in:\")\n",
    "        print(f\"  - {output_path} (con embeddings)\")\n",
    "        print(f\"  - {json_path} (metadata)\")\n",
    "\n",
    "    def load_chunks(self, file_path: str) -> List[DocumentChunk]:\n",
    "        \"\"\"Carica i chunks da file\"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            chunks_data = pickle.load(f)\n",
    "        \n",
    "        chunks = []\n",
    "        for data in chunks_data:\n",
    "            chunk = DocumentChunk(\n",
    "                id=data['id'],\n",
    "                content=data['content'],\n",
    "                page_number=data['page_number'],\n",
    "                chunk_index=data['chunk_index'],\n",
    "                section=data['section'],\n",
    "                subsection=data['subsection'],\n",
    "                document_type=data['document_type'],\n",
    "                embedding=data['embedding'],\n",
    "                metadata=data['metadata']\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def get_stats(self, chunks: List[DocumentChunk]) -> Dict[str, Any]:\n",
    "        \"\"\"Restituisce statistiche sui chunks\"\"\"\n",
    "        if not chunks:\n",
    "            return {}\n",
    "        \n",
    "        stats = {\n",
    "            'total_chunks': len(chunks),\n",
    "            'total_pages': len(set(chunk.page_number for chunk in chunks)),\n",
    "            'avg_chunk_size': np.mean([len(chunk.content) for chunk in chunks]),\n",
    "            'median_chunk_size': np.median([len(chunk.content) for chunk in chunks]),\n",
    "            'sections': {},\n",
    "            'document_types': {},\n",
    "            'chunks_with_embeddings': sum(1 for chunk in chunks if chunk.embedding is not None)\n",
    "        }\n",
    "        \n",
    "        # Conta per sezioni\n",
    "        for chunk in chunks:\n",
    "            stats['sections'][chunk.section] = stats['sections'].get(chunk.section, 0) + 1\n",
    "            stats['document_types'][chunk.document_type] = stats['document_types'].get(chunk.document_type, 0) + 1\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funzione principale per processare il PDF\"\"\"\n",
    "    # Inizializza il processore\n",
    "    processor = PDFEmbeddingProcessor()\n",
    "    \n",
    "    # Percorsi file\n",
    "    pdf_path = r\"C:\\Users\\XM745EF\\OneDrive - EY\\Documents\\AI Academy\\Hackaton-agentic-RAG\\Gruppo_8\\pdf_documents\\Wiki_Conformità_primi_55.pdf\"\n",
    "    output_path = \"chunks_with_embeddings.pkl\"\n",
    "    \n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"File PDF non trovato: {pdf_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processando PDF: {pdf_path}\")\n",
    "    \n",
    "    # Step 1: Estrai testo dal PDF\n",
    "    pages_content = processor.extract_text_from_pdf(pdf_path)\n",
    "    if not pages_content:\n",
    "        print(\"Nessun contenuto estratto dal PDF\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Crea chunks\n",
    "    chunks = processor.create_chunks(pages_content)\n",
    "    if not chunks:\n",
    "        print(\"Nessun chunk creato\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Crea embeddings\n",
    "    chunks_with_embeddings = processor.create_embeddings(chunks)\n",
    "    \n",
    "    # Step 4: Salva risultati\n",
    "    processor.save_chunks(chunks_with_embeddings, output_path)\n",
    "    \n",
    "    # Step 5: Mostra statistiche\n",
    "    stats = processor.get_stats(chunks_with_embeddings)\n",
    "    print(\"\\n=== STATISTICHE FINALI ===\")\n",
    "    print(f\"Chunks totali: {stats['total_chunks']}\")\n",
    "    print(f\"Pagine processate: {stats['total_pages']}\")\n",
    "    print(f\"Dimensione media chunk: {stats['avg_chunk_size']:.0f} caratteri\")\n",
    "    print(f\"Chunks con embeddings: {stats['chunks_with_embeddings']}\")\n",
    "    print(f\"Sezioni identificate: {list(stats['sections'].keys())}\")\n",
    "    print(f\"Tipi documento: {list(stats['document_types'].keys())}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ad9a7",
   "metadata": {},
   "source": [
    "## Chiamata OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a8b65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dei chunk con embeddings...\n",
      "Caricati 201 chunk con embeddings\n",
      "Creazione embedding per il documento corrente...\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Unsupported data type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunks_data:\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Genera embedding per il documento corrente\u001b[39;00m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreazione embedding per il documento corrente...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     query_response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mdocument_text\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Usa i primi 1000 caratteri come query\u001b[39;49;00m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-ada-002\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Assicurati che questo sia il nome corretto del modello\u001b[39;49;00m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     query_embedding = query_response.data[\u001b[32m0\u001b[39m].embedding\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# Trova i chunk più rilevanti\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\resources\\embeddings.py:129\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    123\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    124\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m             ).tolist()\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Unsupported data type"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Carica le variabili d'ambiente dal file .env\n",
    "load_dotenv()\n",
    "\n",
    "# Leggi le chiavi dal file .env\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "azure_endpoint = os.getenv('AZURE_ENDPOINT')\n",
    "api_version = os.getenv('API_VERSION')\n",
    "\n",
    "# Definizione del prompt base\n",
    "BASE_PROMPT = \"\"\"\n",
    "{documento}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt arricchito con RAG\n",
    "RAG_ENHANCED_PROMPT = \"\"\"\n",
    "Ecco alcune informazioni rilevanti dal nostro database di conoscenza:\n",
    "\n",
    "{context}\n",
    "\n",
    "Ora, analizza il seguente documento:\n",
    "\n",
    "{documento}\n",
    "\"\"\"\n",
    "\n",
    "# Crea il client Azure OpenAI\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "# Funzione per calcolare la similarità coseno\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Funzione per recuperare chunk rilevanti in base alla similarità\n",
    "def get_relevant_chunks(query_embedding, chunks, top_k=3):\n",
    "    similarities = []\n",
    "    for chunk in chunks:\n",
    "        if chunk['embedding'] is not None:\n",
    "            similarity = cosine_similarity(query_embedding, chunk['embedding'])\n",
    "            similarities.append((chunk, similarity))\n",
    "    \n",
    "    # Ordina per similarità decrescente\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Prendi i top_k risultati più rilevanti\n",
    "    return [chunk for chunk, _ in similarities[:top_k]]\n",
    "\n",
    "# Carica i chunk con embedding\n",
    "print(\"Caricamento dei chunk con embeddings...\")\n",
    "try:\n",
    "    with open(\"chunks_with_embeddings.pkl\", 'rb') as f:\n",
    "        chunks_data = pickle.load(f)\n",
    "    print(f\"Caricati {len(chunks_data)} chunk con embeddings\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel caricamento dei chunk: {e}\")\n",
    "    chunks_data = []\n",
    "\n",
    "# Testo di esempio da analizzare \n",
    "for file in os.listdir(\"documents\"):\n",
    "    with open(os.path.join(\"documents\", file), \"r\", encoding=\"utf-8\") as f:\n",
    "        document_text = f.read()\n",
    "    \n",
    "    # Se abbiamo chunks con embeddings, miglioriamo il prompt con RAG\n",
    "    if chunks_data:\n",
    "        # Genera embedding per il documento corrente\n",
    "        print(\"Creazione embedding per il documento corrente...\")\n",
    "        query_response = client.embeddings.create(\n",
    "            input=document_text[:1000],  # Usa i primi 1000 caratteri come query\n",
    "            model=\"text-embedding-ada-002\"  # Assicurati che questo sia il nome corretto del modello\n",
    "        )\n",
    "        query_embedding = query_response.data[0].embedding\n",
    "        \n",
    "        # Trova i chunk più rilevanti\n",
    "        relevant_chunks = get_relevant_chunks(query_embedding, chunks_data)\n",
    "        \n",
    "        # Estrai il contenuto dei chunk rilevanti\n",
    "        context = \"\\n\\n\".join([chunk['content'] for chunk in relevant_chunks])\n",
    "        \n",
    "        # Crea il prompt RAG-enhanced\n",
    "        prompt = RAG_ENHANCED_PROMPT.format(context=context, documento=document_text)\n",
    "        print(f\"Prompt arricchito con {len(relevant_chunks)} chunk rilevanti\")\n",
    "    else:\n",
    "        # Usa il prompt base se non abbiamo chunks\n",
    "        prompt = BASE_PROMPT.format(documento=document_text)\n",
    "        print(\"Usando prompt base (senza RAG)\")\n",
    "\n",
    "    # Chiama l'API con il prompt\n",
    "    print(\"Chiamata all'API di Azure OpenAI...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Sei un analizzatore testi professionista. In base alle mail che trovi nel testo che ti passo, fornisci come prima risposta un riepilogo sulle richieste dei clienti. E poi formula una risposta per ogni cliente sulla base della sua richiesta.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_completion_tokens=1024,\n",
    "        temperature=1,\n",
    "    )\n",
    "\n",
    "    print(\"\\nRisposta dall'AI:\")\n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23da38b",
   "metadata": {},
   "source": [
    "## Utility RAG: Funzioni per valutare l'efficacia del sistema RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe31aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"Classe per valutare e visualizzare l'efficacia del sistema RAG\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks_data=None):\n",
    "        \"\"\"Inizializza l'evaluator con i dati dei chunk\"\"\"\n",
    "        self.chunks_data = chunks_data\n",
    "        if chunks_data is None:\n",
    "            try:\n",
    "                with open(\"chunks_with_embeddings.pkl\", 'rb') as f:\n",
    "                    self.chunks_data = pickle.load(f)\n",
    "                print(f\"Caricati {len(self.chunks_data)} chunk con embeddings\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nel caricamento dei chunk: {e}\")\n",
    "                self.chunks_data = []\n",
    "    \n",
    "    def get_embedding(self, text: str, client=None) -> List[float]:\n",
    "        \"\"\"Ottiene l'embedding per un testo\"\"\"\n",
    "        if client is None:\n",
    "            # Assumi che client sia globale\n",
    "            from openai import AzureOpenAI\n",
    "            load_dotenv()\n",
    "            client = AzureOpenAI(\n",
    "                api_key=os.getenv('ADA_API_KEY'),\n",
    "                azure_endpoint=os.getenv('ADA_ENDPOINT'),\n",
    "                api_version=os.getenv('ADA_API_VERSION', \"2024-02-01\")\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                input=text,\n",
    "                model=\"text-embedding-ada-002\"\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'ottenere embedding: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def find_similar_chunks(self, query: str, top_k: int = 5, client=None) -> List[Dict]:\n",
    "        \"\"\"Trova i chunk più simili a una query\"\"\"\n",
    "        # Ottieni embedding per la query\n",
    "        query_embedding = self.get_embedding(query, client)\n",
    "        if query_embedding is None:\n",
    "            return []\n",
    "        \n",
    "        # Calcola similarità con tutti i chunk\n",
    "        similarities = []\n",
    "        for chunk in self.chunks_data:\n",
    "            if chunk['embedding'] is not None:\n",
    "                similarity = self.cosine_similarity(query_embedding, chunk['embedding'])\n",
    "                similarities.append((chunk, similarity))\n",
    "        \n",
    "        # Ordina per similarità decrescente e prendi i top_k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Aggiungi il punteggio di similarità ai chunk\n",
    "        result = []\n",
    "        for chunk, score in similarities[:top_k]:\n",
    "            chunk_with_score = chunk.copy()\n",
    "            chunk_with_score['similarity_score'] = score\n",
    "            result.append(chunk_with_score)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity(a: List[float], b: List[float]) -> float:\n",
    "        \"\"\"Calcola la similarità coseno tra due vettori\"\"\"\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    def visualize_embeddings_2d(self, query=None, query_embedding=None, top_n: int = 100):\n",
    "        \"\"\"Visualizza gli embeddings in 2D usando t-SNE\"\"\"\n",
    "        if not self.chunks_data:\n",
    "            print(\"Nessun chunk disponibile per la visualizzazione\")\n",
    "            return\n",
    "        \n",
    "        # Limita il numero di chunk per la visualizzazione\n",
    "        sample_size = min(top_n, len(self.chunks_data))\n",
    "        sampled_chunks = self.chunks_data[:sample_size]\n",
    "        \n",
    "        # Estrai gli embedding\n",
    "        embeddings = [chunk['embedding'] for chunk in sampled_chunks]\n",
    "        \n",
    "        # Aggiungi query embedding se fornito\n",
    "        if query_embedding is not None:\n",
    "            embeddings.append(query_embedding)\n",
    "        elif query is not None:\n",
    "            query_emb = self.get_embedding(query)\n",
    "            if query_emb:\n",
    "                embeddings.append(query_emb)\n",
    "        \n",
    "        # Applica t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "        reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        # Prepara per la visualizzazione\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Distingui tra query e altri chunk\n",
    "        query_point = None\n",
    "        if query_embedding is not None or query is not None:\n",
    "            query_point = reduced_embeddings[-1]\n",
    "            chunk_points = reduced_embeddings[:-1]\n",
    "        else:\n",
    "            chunk_points = reduced_embeddings\n",
    "        \n",
    "        # Visualizza i chunk\n",
    "        plt.scatter(\n",
    "            chunk_points[:, 0], \n",
    "            chunk_points[:, 1], \n",
    "            alpha=0.6, \n",
    "            c=[chunk.get('section_color', 'blue') for chunk in sampled_chunks]\n",
    "        )\n",
    "        \n",
    "        # Visualizza la query se presente\n",
    "        if query_point is not None:\n",
    "            plt.scatter(\n",
    "                query_point[0], \n",
    "                query_point[1], \n",
    "                marker='*', \n",
    "                s=200, \n",
    "                c='red', \n",
    "                label='Query'\n",
    "            )\n",
    "        \n",
    "        plt.title('t-SNE Visualization of Document Embeddings')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "    \n",
    "    def enhance_prompt_with_rag(self, documento: str, top_k: int = 3, client=None) -> Tuple[str, List[Dict]]:\n",
    "        \"\"\"Arricchisce un prompt con contenuto RAG rilevante\"\"\"\n",
    "        # Template per il prompt RAG\n",
    "        rag_template = \"\"\"\n",
    "Ecco alcune informazioni rilevanti dal nostro database di conoscenza:\n",
    "\n",
    "{context}\n",
    "\n",
    "Ora, analizza il seguente documento:\n",
    "\n",
    "{documento}\n",
    "\"\"\"\n",
    "        \n",
    "        # Ottieni l'embedding per il documento\n",
    "        query_embedding = self.get_embedding(documento[:1000], client)  # Usa i primi 1000 caratteri\n",
    "        if query_embedding is None:\n",
    "            return documento, []\n",
    "        \n",
    "        # Trova chunk rilevanti\n",
    "        relevant_chunks = self.find_similar_chunks(documento[:1000], top_k, client)\n",
    "        \n",
    "        # Se non abbiamo trovato chunk rilevanti, restituisci il documento originale\n",
    "        if not relevant_chunks:\n",
    "            return documento, []\n",
    "        \n",
    "        # Estrai contenuto dai chunk rilevanti\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(relevant_chunks):\n",
    "            context_parts.append(f\"[Fonte {i+1}] (Similarità: {chunk['similarity_score']:.4f})\\n{chunk['content']}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Crea il prompt RAG-enhanced\n",
    "        enhanced_prompt = rag_template.format(context=context, documento=documento)\n",
    "        \n",
    "        return enhanced_prompt, relevant_chunks\n",
    "    \n",
    "    def evaluate_rag_effectiveness(self, queries: List[str], top_k_values: List[int] = [1, 3, 5, 10]):\n",
    "        \"\"\"Valuta l'efficacia del RAG per diverse query e configurazioni\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for query in queries:\n",
    "            query_result = {'query': query, 'top_k_results': {}}\n",
    "            print(f\"Valutando query: '{query[:50]}...'\")\n",
    "            \n",
    "            for top_k in top_k_values:\n",
    "                start_time = time.time()\n",
    "                enhanced_prompt, relevant_chunks = self.enhance_prompt_with_rag(query, top_k)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                query_result['top_k_results'][top_k] = {\n",
    "                    'time_taken': end_time - start_time,\n",
    "                    'num_chunks': len(relevant_chunks),\n",
    "                    'avg_similarity': np.mean([chunk['similarity_score'] for chunk in relevant_chunks]) if relevant_chunks else 0,\n",
    "                    'relevant_chunk_ids': [chunk['id'] for chunk in relevant_chunks]\n",
    "                }\n",
    "                \n",
    "                print(f\"  - Top {top_k}: trovati {len(relevant_chunks)} chunk, \"\n",
    "                     f\"similarità media: {query_result['top_k_results'][top_k]['avg_similarity']:.4f}\")\n",
    "            \n",
    "            results[query[:20]] = query_result\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_evaluation_results(self, results, filename=\"rag_evaluation_results.json\"):\n",
    "        \"\"\"Salva i risultati della valutazione in un file JSON\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Risultati salvati in {filename}\")\n",
    "\n",
    "# Demo per testare il sistema RAG\n",
    "def test_rag_system():\n",
    "    # Inizializza l'evaluator\n",
    "    evaluator = RAGEvaluator()\n",
    "    \n",
    "    # Esempio di query da testare\n",
    "    test_query = \"\"\"\n",
    "    Sono interessato alle norme sulla protezione dei dati personali e sulla gestione dei dati sensibili nell'ambito del GDPR.\n",
    "    Potresti fornirmi informazioni sugli obblighi di conformità e sulle possibili sanzioni in caso di violazioni?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ottieni e stampa i chunk più simili\n",
    "    similar_chunks = evaluator.find_similar_chunks(test_query, top_k=3)\n",
    "    print(f\"Trovati {len(similar_chunks)} chunk simili alla query.\\n\")\n",
    "    \n",
    "    for i, chunk in enumerate(similar_chunks):\n",
    "        print(f\"Chunk #{i+1} (ID: {chunk['id']}, Score: {chunk['similarity_score']:.4f}):\")\n",
    "        print(f\"Pagina: {chunk['page_number']}, Sezione: {chunk['section']}\")\n",
    "        print(f\"Contenuto: {chunk['content'][:150]}...\\n\")\n",
    "    \n",
    "    # Arricchisci il prompt con RAG\n",
    "    enhanced_prompt, _ = evaluator.enhance_prompt_with_rag(test_query)\n",
    "    print(\"Prompt arricchito con RAG:\")\n",
    "    print(enhanced_prompt[:500] + \"...\\n\")\n",
    "    \n",
    "    # Visualizza gli embedding\n",
    "    evaluator.visualize_embeddings_2d(test_query, top_n=50)\n",
    "\n",
    "# Decommentare per eseguire il test\n",
    "# test_rag_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae5295",
   "metadata": {},
   "source": [
    "## Implementazione RAG per DOCUMENT_TYPE_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e70f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import time\n",
    "\n",
    "# Carica le variabili d'ambiente dal file .env\n",
    "load_dotenv()\n",
    "\n",
    "# Chiavi di Azure OpenAI\n",
    "azure_chat_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_chat_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_chat_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "azure_chat_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# Configurazione embedding\n",
    "azure_embedding_api_key = os.getenv(\"ADA_API_KEY\")\n",
    "azure_embedding_endpoint = os.getenv(\"ADA_ENDPOINT\")\n",
    "azure_embedding_api_version = os.getenv(\"ADA_API_VERSION\", \"2024-02-01\")\n",
    "azure_embedding_deployment = os.getenv(\"ADA_DEPLOYMENT_NAME\", \"text-embedding-ada-002\")\n",
    "\n",
    "# Crea i client di Azure OpenAI\n",
    "chat_client = openai.AzureOpenAI(\n",
    "    api_key=azure_chat_api_key,\n",
    "    azure_endpoint=azure_chat_endpoint,\n",
    "    api_version=azure_chat_api_version\n",
    ")\n",
    "\n",
    "embedding_client = openai.AzureOpenAI(\n",
    "    api_key=azure_embedding_api_key, \n",
    "    azure_endpoint=azure_embedding_endpoint,\n",
    "    api_version=azure_embedding_api_version\n",
    ")\n",
    "\n",
    "# Carica i chunks con embedding\n",
    "print(\"Caricamento dei chunk con embeddings...\")\n",
    "try:\n",
    "    with open(\"chunks_with_embeddings.pkl\", 'rb') as f:\n",
    "        chunks_data = pickle.load(f)\n",
    "    print(f\"Caricati {len(chunks_data)} chunk con embeddings\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore nel caricamento dei chunk: {e}\")\n",
    "    chunks_data = []\n",
    "\n",
    "# Definizione dei prompt\n",
    "# Prompt originale\n",
    "DOCUMENT_TYPE_PROMPT = \"\"\"\n",
    "Ti fornirò un documento aziendale.\n",
    " \n",
    "Il tuo compito è determinare il tipo di documento, scegliendo tra i seguenti: \n",
    "- Mail\n",
    "- Nota di credito\n",
    "- Ordine di acquisto\n",
    "- Contratto\n",
    "- Altro\n",
    " \n",
    "Devi basarti solo sul contenuto del documento fornito.\n",
    " \n",
    "Ora incollerò il contenuto del documento tra tripli apici. Rispondi semplicemente con il tipo, nulla di più.\n",
    " \n",
    "Documento:\n",
    "'''\n",
    "{documento}\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "# Prompt arricchito con RAG\n",
    "RAG_DOCUMENT_TYPE_PROMPT = \"\"\"\n",
    "Ti fornirò un documento aziendale.\n",
    "\n",
    "Il tuo compito è determinare il tipo di documento, scegliendo tra i seguenti: \n",
    "- Mail\n",
    "- Nota di credito\n",
    "- Ordine di acquisto\n",
    "- Contratto\n",
    "- Altro\n",
    "\n",
    "Devi basarti principalmente sul contenuto del documento fornito.\n",
    "\n",
    "Per aiutarti, ecco alcune informazioni rilevanti dal nostro database di conoscenza che potrebbero aiutarti nell'analisi:\n",
    "\n",
    "{context}\n",
    "\n",
    "Ora incollerò il contenuto del documento tra tripli apici. Rispondi semplicemente con il tipo, nulla di più.\n",
    "\n",
    "Documento:\n",
    "'''\n",
    "{documento}\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "# Funzione per calcolare la similarità coseno\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Funzione per recuperare chunk rilevanti\n",
    "def get_relevant_chunks(query_embedding, chunks_data, top_k=3):\n",
    "    if not chunks_data:\n",
    "        return []\n",
    "    \n",
    "    similarities = []\n",
    "    for chunk in chunks_data:\n",
    "        if chunk['embedding'] is not None:\n",
    "            similarity = cosine_similarity(query_embedding, chunk['embedding'])\n",
    "            similarities.append((chunk, similarity))\n",
    "    \n",
    "    # Ordina per similarità decrescente\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Prendi i top_k risultati più rilevanti\n",
    "    return [(chunk, sim) for chunk, sim in similarities[:top_k]]\n",
    "\n",
    "# Funzione per generare l'embedding di un testo\n",
    "def get_embedding(text):\n",
    "    try:\n",
    "        response = embedding_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=azure_embedding_deployment\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel generare embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Funzione per classificare un documento con RAG\n",
    "def classify_document_with_rag(document_text, use_rag=True, top_k=3):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_rag and chunks_data:\n",
    "        # Genera embedding per il documento\n",
    "        print(\"Generazione embedding per il documento...\")\n",
    "        doc_embedding = get_embedding(document_text[:1000])  # Usa i primi 1000 caratteri\n",
    "        \n",
    "        if doc_embedding:\n",
    "            # Trova chunk rilevanti\n",
    "            print(f\"Ricerca dei {top_k} chunk più rilevanti...\")\n",
    "            relevant_chunks = get_relevant_chunks(doc_embedding, chunks_data, top_k)\n",
    "            \n",
    "            # Estrai contenuto e punteggi\n",
    "            context_parts = []\n",
    "            for i, (chunk, similarity) in enumerate(relevant_chunks):\n",
    "                context_parts.append(\n",
    "                    f\"[FONTE {i+1}] (Similitudine: {similarity:.4f}, Sezione: {chunk.get('section', 'N/A')})\\n\"\n",
    "                    f\"{chunk['content']}\"\n",
    "                )\n",
    "            \n",
    "            context = \"\\n\\n\".join(context_parts)\n",
    "            prompt = RAG_DOCUMENT_TYPE_PROMPT.format(context=context, documento=document_text)\n",
    "            print(f\"Prompt arricchito con {len(relevant_chunks)} fonti dalla knowledge base\")\n",
    "        else:\n",
    "            print(\"Impossibile generare embedding, uso il prompt standard\")\n",
    "            prompt = DOCUMENT_TYPE_PROMPT.format(documento=document_text)\n",
    "    else:\n",
    "        prompt = DOCUMENT_TYPE_PROMPT.format(documento=document_text)\n",
    "        print(\"Usando prompt standard (senza RAG)\")\n",
    "    \n",
    "    # Chiamata all'API\n",
    "    print(\"Chiamata all'API di Azure OpenAI per classificazione...\")\n",
    "    response = chat_client.chat.completions.create(\n",
    "        model=azure_chat_deployment,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Sei un analista di documenti aziendali esperto.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_completion_tokens=256,\n",
    "        temperature=0.3,  # Temperatura più bassa per risposta più consistente\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Calcola il tempo impiegato\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Classificazione completata in {elapsed_time:.2f} secondi\")\n",
    "    \n",
    "    return result, prompt\n",
    "\n",
    "# Test di esempio con diversi file nella cartella \"anonymized\"\n",
    "def test_document_classification(use_rag=True):\n",
    "    results = []\n",
    "    \n",
    "    # Trova i file nella directory anonymized\n",
    "    directory = \"anonymized\"\n",
    "    if not os.path.isdir(directory):\n",
    "        print(f\"La directory '{directory}' non esiste.\")\n",
    "        return\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    for file in files[:3]:  # Limita a 3 file per test\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                document_text = f.read()\n",
    "            \n",
    "            print(f\"\\nAnalisi del file: {file}\")\n",
    "            document_type, _ = classify_document_with_rag(document_text, use_rag=use_rag)\n",
    "            \n",
    "            results.append({\n",
    "                \"file\": file,\n",
    "                \"type\": document_type\n",
    "            })\n",
    "            \n",
    "            print(f\"Tipo documento rilevato: {document_type}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'elaborazione del file {file}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Esegui il test - commenta/decommenta per eseguire\n",
    "# test_results = test_document_classification(use_rag=True)\n",
    "\n",
    "# Esempio di comparazione RAG vs No-RAG\n",
    "def compare_rag_vs_no_rag():\n",
    "    print(\"Comparazione RAG vs No-RAG per la classificazione dei documenti\")\n",
    "    \n",
    "    # Scegli un file di esempio\n",
    "    file_path = None\n",
    "    if os.path.isdir(\"anonymized\") and os.listdir(\"anonymized\"):\n",
    "        file_path = os.path.join(\"anonymized\", os.listdir(\"anonymized\")[0])\n",
    "    elif os.path.isdir(\"documents\") and os.listdir(\"documents\"):\n",
    "        file_path = os.path.join(\"documents\", os.listdir(\"documents\")[0])\n",
    "    \n",
    "    if not file_path:\n",
    "        print(\"Nessun file trovato per il test\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            document_text = f.read()\n",
    "        \n",
    "        print(f\"\\nAnalisi del file: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Test senza RAG\n",
    "        print(\"\\n--- CLASSIFICAZIONE SENZA RAG ---\")\n",
    "        no_rag_type, no_rag_prompt = classify_document_with_rag(document_text, use_rag=False)\n",
    "        \n",
    "        # Test con RAG\n",
    "        print(\"\\n--- CLASSIFICAZIONE CON RAG ---\")\n",
    "        rag_type, rag_prompt = classify_document_with_rag(document_text, use_rag=True)\n",
    "        \n",
    "        print(\"\\n--- RISULTATI ---\")\n",
    "        print(f\"Tipo documento (senza RAG): {no_rag_type}\")\n",
    "        print(f\"Tipo documento (con RAG): {rag_type}\")\n",
    "        \n",
    "        return {\n",
    "            \"file\": os.path.basename(file_path),\n",
    "            \"no_rag_type\": no_rag_type,\n",
    "            \"rag_type\": rag_type,\n",
    "            \"no_rag_prompt_length\": len(no_rag_prompt),\n",
    "            \"rag_prompt_length\": len(rag_prompt)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il confronto RAG vs No-RAG: {e}\")\n",
    "        return None\n",
    "\n",
    "# Decommentare per eseguire la comparazione\n",
    "# comparison_result = compare_rag_vs_no_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1addbe",
   "metadata": {},
   "source": [
    "## Esempio Pratico: Analisi di Documenti con RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f9d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta this per eseguire l'esempio pratico\n",
    "run_practical_example = True  # Modifica a True per eseguire\n",
    "\n",
    "if run_practical_example:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm.notebook import tqdm\n",
    "    import time\n",
    "    \n",
    "    # Configurazione dell'esperimento\n",
    "    print(\"Avvio dell'analisi comparativa RAG vs Non-RAG...\")\n",
    "    \n",
    "    # Ottieni lista dei file da processare\n",
    "    directory = \"anonymized\" if os.path.isdir(\"anonymized\") else \"documents\"\n",
    "    files = os.listdir(directory)[:5]  # Limita a 5 file per brevità\n",
    "    \n",
    "    # Inizializza strutture dati per risultati\n",
    "    results = []\n",
    "    \n",
    "    # Processa ogni file con e senza RAG\n",
    "    for file in tqdm(files, desc=\"Analisi documenti\"):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                document_text = f.read()\n",
    "            \n",
    "            # Analisi senza RAG\n",
    "            start_time = time.time()\n",
    "            no_rag_type, no_rag_prompt = classify_document_with_rag(document_text, use_rag=False)\n",
    "            no_rag_time = time.time() - start_time\n",
    "            \n",
    "            # Analisi con RAG\n",
    "            start_time = time.time()\n",
    "            rag_type, rag_prompt = classify_document_with_rag(document_text, use_rag=True, top_k=3)\n",
    "            rag_time = time.time() - start_time\n",
    "            \n",
    "            # Salva risultati\n",
    "            results.append({\n",
    "                \"file\": file,\n",
    "                \"no_rag_type\": no_rag_type,\n",
    "                \"rag_type\": rag_type,\n",
    "                \"no_rag_time\": no_rag_time,\n",
    "                \"rag_time\": rag_time,\n",
    "                \"no_rag_prompt_length\": len(no_rag_prompt),\n",
    "                \"rag_prompt_length\": len(rag_prompt),\n",
    "                \"match\": no_rag_type == rag_type\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'analisi di {file}: {e}\")\n",
    "    \n",
    "    # Crea dataframe con risultati\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Statistiche sui risultati\n",
    "    print(\"\\n=== RISULTATI DELL'ANALISI COMPARATIVA ===\")\n",
    "    print(f\"Documenti analizzati: {len(df)}\")\n",
    "    print(f\"Concordanza classificazioni: {df['match'].sum()}/{len(df)} ({df['match'].mean()*100:.1f}%)\")\n",
    "    print(f\"Tempo medio senza RAG: {df['no_rag_time'].mean():.2f} secondi\")\n",
    "    print(f\"Tempo medio con RAG: {df['rag_time'].mean():.2f} secondi\")\n",
    "    print(f\"Overhead medio RAG: {(df['rag_time'].mean() / df['no_rag_time'].mean() - 1) * 100:.1f}%\")\n",
    "    \n",
    "    # Visualizzazione dei risultati\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Confronto tempi di esecuzione\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(['Standard', 'RAG'], [df['no_rag_time'].mean(), df['rag_time'].mean()], color=['blue', 'orange'])\n",
    "    plt.title('Tempo medio di classificazione')\n",
    "    plt.ylabel('Secondi')\n",
    "    \n",
    "    # Confronto lunghezza prompt\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(['Standard', 'RAG'], [df['no_rag_prompt_length'].mean(), df['rag_prompt_length'].mean()], color=['blue', 'orange'])\n",
    "    plt.title('Lunghezza media prompt')\n",
    "    plt.ylabel('Caratteri')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Dettaglio per ogni documento\n",
    "    print(\"\\n--- CLASSIFICAZIONE PER DOCUMENTO ---\")\n",
    "    for _, row in df.iterrows():\n",
    "        match_symbol = \"✓\" if row[\"match\"] else \"✗\" \n",
    "        print(f\"{row['file']}: Standard: {row['no_rag_type']} | RAG: {row['rag_type']} {match_symbol}\")\n",
    "else:\n",
    "    print(\"Per eseguire l'esempio pratico, imposta 'run_practical_example = True' e riesegui la cella.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d2f80",
   "metadata": {},
   "source": [
    "## Implementazione di un Workflow Agentico per RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# Carica le variabili d'ambiente\n",
    "load_dotenv()\n",
    "\n",
    "# Configurazione di Azure OpenAI\n",
    "AZURE_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "AZURE_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "ADA_API_KEY = os.getenv(\"ADA_API_KEY\")\n",
    "ADA_ENDPOINT = os.getenv(\"ADA_ENDPOINT\")\n",
    "ADA_API_VERSION = os.getenv(\"ADA_API_VERSION\", \"2024-02-01\")\n",
    "ADA_DEPLOYMENT = os.getenv(\"ADA_DEPLOYMENT_NAME\", \"text-embedding-ada-002\")\n",
    "\n",
    "# Inizializza i client di OpenAI\n",
    "chat_client = openai.AzureOpenAI(\n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")\n",
    "\n",
    "embedding_client = openai.AzureOpenAI(\n",
    "    api_key=ADA_API_KEY,\n",
    "    azure_endpoint=ADA_ENDPOINT,\n",
    "    api_version=ADA_API_VERSION\n",
    ")\n",
    "\n",
    "# Definizione delle strutture dati per l'agente\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Rappresenta un documento da analizzare\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    file_path: Optional[str] = None\n",
    "    file_name: Optional[str] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    doc_type: Optional[str] = None\n",
    "    embedding: Optional[List[float]] = None\n",
    "    classification_confidence: float = 0.0\n",
    "    processed_timestamp: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class KnowledgeChunk:\n",
    "    \"\"\"Rappresenta un chunk di conoscenza dal database\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    embedding: List[float]\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    relevance_score: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class ReasoningStep:\n",
    "    \"\"\"Rappresenta uno step di ragionamento dell'agente\"\"\"\n",
    "    id: str\n",
    "    thought: str\n",
    "    action: str\n",
    "    action_input: Dict[str, Any]\n",
    "    action_output: Any = None\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Rappresenta lo stato corrente dell'agente\"\"\"\n",
    "    agent_id: str\n",
    "    document: Optional[Document] = None\n",
    "    relevant_chunks: List[KnowledgeChunk] = field(default_factory=list)\n",
    "    reasoning_steps: List[ReasoningStep] = field(default_factory=list)\n",
    "    final_decision: Optional[str] = None\n",
    "    confidence: float = 0.0\n",
    "    start_time: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "    end_time: Optional[str] = None\n",
    "    status: str = \"initialized\"  # initialized, processing, completed, failed\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class AgenticRAG:\n",
    "    \"\"\"Implementa un sistema RAG con capacità agentiche\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_base_path: str = \"chunks_with_embeddings.pkl\"):\n",
    "        \"\"\"Inizializza il sistema RAG agentico\"\"\"\n",
    "        self.knowledge_chunks = self._load_knowledge_base(knowledge_base_path)\n",
    "        print(f\"Knowledge base caricata: {len(self.knowledge_chunks)} chunk disponibili\")\n",
    "    \n",
    "    def _load_knowledge_base(self, knowledge_base_path: str) -> List[KnowledgeChunk]:\n",
    "        \"\"\"Carica i chunk dalla knowledge base\"\"\"\n",
    "        try:\n",
    "            with open(knowledge_base_path, 'rb') as f:\n",
    "                raw_chunks = pickle.load(f)\n",
    "            \n",
    "            # Converti in KnowledgeChunk\n",
    "            knowledge_chunks = []\n",
    "            for chunk in raw_chunks:\n",
    "                if chunk.get('embedding') is not None:\n",
    "                    knowledge_chunks.append(\n",
    "                        KnowledgeChunk(\n",
    "                            id=chunk.get('id', str(uuid.uuid4())),\n",
    "                            content=chunk.get('content', ''),\n",
    "                            embedding=chunk.get('embedding', []),\n",
    "                            metadata=chunk.get('metadata', {})\n",
    "                        )\n",
    "                    )\n",
    "            \n",
    "            return knowledge_chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel caricamento della knowledge base: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def create_embedding(self, text: str) -> Optional[List[float]]:\n",
    "        \"\"\"Genera embedding per un testo\"\"\"\n",
    "        try:\n",
    "            # Pulisci e tronca il testo se necessario\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', text.strip())\n",
    "            if len(cleaned_text) > 8000:\n",
    "                cleaned_text = cleaned_text[:8000]  # Limite per evitare errori API\n",
    "            \n",
    "            response = embedding_client.embeddings.create(\n",
    "                input=cleaned_text,\n",
    "                model=ADA_DEPLOYMENT\n",
    "            )\n",
    "            \n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella creazione dell'embedding: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def cosine_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:\n",
    "        \"\"\"Calcola la similarità coseno tra due embedding\"\"\"\n",
    "        return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, document: Document, top_k: int = 5) -> List[KnowledgeChunk]:\n",
    "        \"\"\"Recupera i chunk più rilevanti per il documento\"\"\"\n",
    "        if document.embedding is None:\n",
    "            document.embedding = self.create_embedding(document.content[:1500])\n",
    "            if document.embedding is None:\n",
    "                return []\n",
    "        \n",
    "        # Calcola similarità con tutti i chunk\n",
    "        chunk_similarities = []\n",
    "        for chunk in self.knowledge_chunks:\n",
    "            similarity = self.cosine_similarity(document.embedding, chunk.embedding)\n",
    "            chunk_with_score = KnowledgeChunk(\n",
    "                id=chunk.id,\n",
    "                content=chunk.content,\n",
    "                embedding=chunk.embedding,\n",
    "                metadata=chunk.metadata,\n",
    "                relevance_score=similarity\n",
    "            )\n",
    "            chunk_similarities.append((chunk_with_score, similarity))\n",
    "        \n",
    "        # Ordina per similarità decrescente\n",
    "        chunk_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Prendi i top_k risultati e aggiorna i punteggi\n",
    "        top_chunks = [chunk for chunk, _ in chunk_similarities[:top_k]]\n",
    "        return top_chunks\n",
    "    \n",
    "    def _generate_system_prompt(self) -> str:\n",
    "        \"\"\"Genera il prompt di sistema per l'agente\"\"\"\n",
    "        return \"\"\"Sei un assistente AI avanzato che analizza documenti aziendali.\n",
    "Il tuo compito è determinare il tipo di documento e ragionare in modo chiaro e strutturato.\n",
    "Per ogni documento devi:\n",
    "1. Analizzare attentamente il contenuto\n",
    "2. Considerare le informazioni fornite dalla knowledge base\n",
    "3. Identificare pattern e caratteristiche chiave\n",
    "4. Formulare un ragionamento strutturato\n",
    "5. Prendere una decisione basata sui fatti\n",
    "\n",
    "Risponderai in formato JSON con i seguenti campi:\n",
    "- thoughts: il tuo ragionamento dettagliato\n",
    "- observations: osservazioni rilevanti sul documento\n",
    "- classification: il tipo di documento (Mail, Nota di credito, Ordine di acquisto, Contratto, Altro)\n",
    "- confidence: un valore da 0 a 1 che indica la tua sicurezza nella classificazione\n",
    "- nextAction: l'azione successiva consigliata (es. \"extract_info\", \"request_more_context\", \"final_decision\")\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate_user_prompt(self, document: Document, relevant_chunks: List[KnowledgeChunk]) -> str:\n",
    "        \"\"\"Genera il prompt utente con il documento e i chunk rilevanti\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        # Aggiungi i chunk rilevanti come contesto\n",
    "        for i, chunk in enumerate(relevant_chunks):\n",
    "            context_parts.append(\n",
    "                f\"[FONTE {i+1}] (Rilevanza: {chunk.relevance_score:.4f})\\n{chunk.content}\"\n",
    "            )\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts) if context_parts else \"Nessun contesto rilevante disponibile.\"\n",
    "        \n",
    "        return f\"\"\"# CONTESTO DALLA KNOWLEDGE BASE\n",
    "{context}\n",
    "\n",
    "# DOCUMENTO DA ANALIZZARE\n",
    "```\n",
    "{document.content}\n",
    "```\n",
    "\n",
    "Analizza questo documento e determina il suo tipo, scegliendo tra: Mail, Nota di credito, Ordine di acquisto, Contratto, Altro.\n",
    "Fornisci un ragionamento dettagliato sulla tua classificazione e proponi l'azione successiva appropriata.\n",
    "\"\"\"\n",
    "    \n",
    "    def parse_agent_response(self, response_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analizza la risposta dell'agente e la converte in un dizionario strutturato\"\"\"\n",
    "        # Cerca di trovare JSON nella risposta\n",
    "        json_match = re.search(r'```json\\n(.*?)\\n```', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group(1)\n",
    "        else:\n",
    "            # Cerca JSON senza delimitatori di codice\n",
    "            json_match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(1)\n",
    "            else:\n",
    "                # Fallback: estrai campi chiave dal testo\n",
    "                return self._extract_fields_from_text(response_text)\n",
    "        \n",
    "        # Prova a parsare il JSON\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel parsing JSON: {e}\")\n",
    "            return self._extract_fields_from_text(response_text)\n",
    "    \n",
    "    def _extract_fields_from_text(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Estrai campi chiave dal testo quando il parsing JSON fallisce\"\"\"\n",
    "        result = {\n",
    "            \"thoughts\": \"\",\n",
    "            \"observations\": \"\",\n",
    "            \"classification\": \"Altro\",\n",
    "            \"confidence\": 0.5,\n",
    "            \"nextAction\": \"final_decision\"\n",
    "        }\n",
    "        \n",
    "        # Estrai classificazione\n",
    "        classification_match = re.search(r'classificazione:?\\s*(Mail|Nota di credito|Ordine di acquisto|Contratto|Altro)', text, re.IGNORECASE)\n",
    "        if classification_match:\n",
    "            result[\"classification\"] = classification_match.group(1)\n",
    "        \n",
    "        # Estrai confidence se presente\n",
    "        confidence_match = re.search(r'confidence:?\\s*(\\d+\\.?\\d*)', text)\n",
    "        if confidence_match:\n",
    "            try:\n",
    "                result[\"confidence\"] = float(confidence_match.group(1))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Estrai thoughts\n",
    "        thoughts_match = re.search(r'thoughts:?\\s*(.*?)(?:observations|classification|confidence|nextAction|\\Z)', text, re.DOTALL | re.IGNORECASE)\n",
    "        if thoughts_match:\n",
    "            result[\"thoughts\"] = thoughts_match.group(1).strip()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def process_document(self, document_content: str, document_id: Optional[str] = None, metadata: Dict[str, Any] = None) -> AgentState:\n",
    "        \"\"\"Processa un documento dall'inizio alla fine\"\"\"\n",
    "        agent_id = str(uuid.uuid4())\n",
    "        agent_state = AgentState(agent_id=agent_id)\n",
    "        \n",
    "        try:\n",
    "            # Crea l'oggetto documento\n",
    "            document = Document(\n",
    "                id=document_id or str(uuid.uuid4()),\n",
    "                content=document_content,\n",
    "                metadata=metadata or {},\n",
    "                processed_timestamp=datetime.now().isoformat()\n",
    "            )\n",
    "            agent_state.document = document\n",
    "            agent_state.status = \"processing\"\n",
    "            \n",
    "            # Step 1: Genera embedding per il documento\n",
    "            print(f\"[Agente {agent_id[:8]}] Generazione embedding per il documento...\")\n",
    "            document.embedding = self.create_embedding(document.content[:1500])\n",
    "            \n",
    "            if document.embedding is None:\n",
    "                agent_state.status = \"failed\"\n",
    "                agent_state.error = \"Impossibile generare embedding per il documento\"\n",
    "                return agent_state\n",
    "            \n",
    "            # Step 2: Recupera chunk rilevanti\n",
    "            print(f\"[Agente {agent_id[:8]}] Recupero chunk rilevanti...\")\n",
    "            relevant_chunks = self.retrieve_relevant_chunks(document)\n",
    "            agent_state.relevant_chunks = relevant_chunks\n",
    "            \n",
    "            # Registra lo step di ragionamento\n",
    "            agent_state.reasoning_steps.append(\n",
    "                ReasoningStep(\n",
    "                    id=\"step_retrieval\",\n",
    "                    thought=\"Recupero informazioni rilevanti dalla knowledge base\",\n",
    "                    action=\"retrieve_chunks\",\n",
    "                    action_input={\"document_id\": document.id},\n",
    "                    action_output=f\"Recuperati {len(relevant_chunks)} chunk rilevanti\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Step 3: Genera prompt per l'agente\n",
    "            system_prompt = self._generate_system_prompt()\n",
    "            user_prompt = self._generate_user_prompt(document, relevant_chunks)\n",
    "            \n",
    "            # Step 4: Chiama l'API per il ragionamento e la classificazione\n",
    "            print(f\"[Agente {agent_id[:8]}] Analisi del documento in corso...\")\n",
    "            response = chat_client.chat.completions.create(\n",
    "                model=AZURE_DEPLOYMENT,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            response_text = response.choices[0].message.content\n",
    "            parsed_response = self.parse_agent_response(response_text)\n",
    "            \n",
    "            # Registra lo step di ragionamento\n",
    "            agent_state.reasoning_steps.append(\n",
    "                ReasoningStep(\n",
    "                    id=\"step_analysis\",\n",
    "                    thought=parsed_response.get(\"thoughts\", \"\"),\n",
    "                    action=\"analyze_document\",\n",
    "                    action_input={\"document_id\": document.id},\n",
    "                    action_output=parsed_response\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Step 5: Prendi la decisione finale\n",
    "            document.doc_type = parsed_response.get(\"classification\", \"Altro\")\n",
    "            document.classification_confidence = parsed_response.get(\"confidence\", 0.5)\n",
    "            \n",
    "            agent_state.final_decision = document.doc_type\n",
    "            agent_state.confidence = document.classification_confidence\n",
    "            agent_state.status = \"completed\"\n",
    "            agent_state.end_time = datetime.now().isoformat()\n",
    "            \n",
    "            # Step 6: Se il sistema suggerisce un'azione successiva, registrala\n",
    "            next_action = parsed_response.get(\"nextAction\")\n",
    "            if next_action and next_action != \"final_decision\":\n",
    "                agent_state.reasoning_steps.append(\n",
    "                    ReasoningStep(\n",
    "                        id=\"step_next_action\",\n",
    "                        thought=f\"L'analisi suggerisce un'azione successiva: {next_action}\",\n",
    "                        action=next_action,\n",
    "                        action_input={\"document_id\": document.id},\n",
    "                        action_output=None  # Da eseguire in seguito\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            return agent_state\n",
    "            \n",
    "        except Exception as e:\n",
    "            agent_state.status = \"failed\"\n",
    "            agent_state.error = str(e)\n",
    "            agent_state.end_time = datetime.now().isoformat()\n",
    "            print(f\"Errore durante il processing: {e}\")\n",
    "            return agent_state\n",
    "    \n",
    "    def save_agent_state(self, agent_state: AgentState, output_path: str):\n",
    "        \"\"\"Salva lo stato dell'agente su file\"\"\"\n",
    "        try:\n",
    "            # Converti in dizionario serializzabile\n",
    "            state_dict = asdict(agent_state)\n",
    "            \n",
    "            # Rimuovi gli embedding che non sono serializzabili in JSON\n",
    "            if state_dict.get(\"document\") and state_dict[\"document\"].get(\"embedding\"):\n",
    "                state_dict[\"document\"][\"embedding\"] = None\n",
    "            \n",
    "            for chunk in state_dict.get(\"relevant_chunks\", []):\n",
    "                if chunk.get(\"embedding\"):\n",
    "                    chunk[\"embedding\"] = None\n",
    "            \n",
    "            # Salva su file\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(state_dict, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"Stato dell'agente salvato in {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel salvare lo stato dell'agente: {e}\")\n",
    "    \n",
    "    def format_agent_report(self, agent_state: AgentState) -> str:\n",
    "        \"\"\"Formatta un report leggibile dello stato dell'agente\"\"\"\n",
    "        if not agent_state:\n",
    "            return \"Nessuno stato agente disponibile.\"\n",
    "        \n",
    "        # Estrai informazioni principali\n",
    "        report = []\n",
    "        report.append(f\"# Rapporto Analisi Documento\")\n",
    "        report.append(f\"ID Sessione: {agent_state.agent_id}\")\n",
    "        report.append(f\"Stato: {agent_state.status}\")\n",
    "        \n",
    "        if agent_state.document:\n",
    "            report.append(f\"\\n## Documento\")\n",
    "            report.append(f\"ID: {agent_state.document.id}\")\n",
    "            if agent_state.document.file_name:\n",
    "                report.append(f\"Nome file: {agent_state.document.file_name}\")\n",
    "            \n",
    "            # Mostra un'anteprima del contenuto\n",
    "            doc_preview = agent_state.document.content[:200] + \"...\" if len(agent_state.document.content) > 200 else agent_state.document.content\n",
    "            report.append(f\"\\nAnteprima contenuto:\\n```\\n{doc_preview}\\n```\")\n",
    "        \n",
    "        # Decisione finale\n",
    "        if agent_state.final_decision:\n",
    "            report.append(f\"\\n## Decisione Finale\")\n",
    "            report.append(f\"Tipo documento: **{agent_state.final_decision}**\")\n",
    "            report.append(f\"Confidenza: {agent_state.confidence:.2f}\")\n",
    "        \n",
    "        # Knowledge chunks\n",
    "        if agent_state.relevant_chunks:\n",
    "            report.append(f\"\\n## Fonti di Conoscenza Rilevanti\")\n",
    "            for i, chunk in enumerate(agent_state.relevant_chunks[:3]):  # Mostra solo i primi 3\n",
    "                report.append(f\"\\n### Fonte {i+1} (Score: {chunk.relevance_score:.4f})\")\n",
    "                chunk_preview = chunk.content[:150] + \"...\" if len(chunk.content) > 150 else chunk.content\n",
    "                report.append(f\"```\\n{chunk_preview}\\n```\")\n",
    "            \n",
    "            if len(agent_state.relevant_chunks) > 3:\n",
    "                report.append(f\"\\n... e altre {len(agent_state.relevant_chunks) - 3} fonti\")\n",
    "        \n",
    "        # Ragionamento\n",
    "        if agent_state.reasoning_steps:\n",
    "            report.append(f\"\\n## Processo di Ragionamento\")\n",
    "            for i, step in enumerate(agent_state.reasoning_steps):\n",
    "                report.append(f\"\\n### Step {i+1}: {step.action}\")\n",
    "                report.append(step.thought)\n",
    "                \n",
    "                # Se disponibile, mostra l'output del ragionamento più dettagliato\n",
    "                if step.action == \"analyze_document\" and isinstance(step.action_output, dict):\n",
    "                    thoughts = step.action_output.get(\"thoughts\")\n",
    "                    observations = step.action_output.get(\"observations\")\n",
    "                    \n",
    "                    if thoughts:\n",
    "                        report.append(f\"\\nRagionamento dettagliato:\")\n",
    "                        report.append(f\"```\\n{thoughts}\\n```\")\n",
    "                    \n",
    "                    if observations:\n",
    "                        report.append(f\"\\nOsservazioni:\")\n",
    "                        report.append(f\"```\\n{observations}\\n```\")\n",
    "        \n",
    "        # Calcola tempo totale\n",
    "        if agent_state.start_time and agent_state.end_time:\n",
    "            start = datetime.fromisoformat(agent_state.start_time)\n",
    "            end = datetime.fromisoformat(agent_state.end_time)\n",
    "            duration = (end - start).total_seconds()\n",
    "            report.append(f\"\\n## Statistiche\")\n",
    "            report.append(f\"Tempo di analisi: {duration:.2f} secondi\")\n",
    "        \n",
    "        # Errore (se presente)\n",
    "        if agent_state.error:\n",
    "            report.append(f\"\\n## Errore\")\n",
    "            report.append(f\"```\\n{agent_state.error}\\n```\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "# Funzione di test per l'AgenticRAG\n",
    "def test_agentic_rag_system():\n",
    "    # Inizializza il sistema RAG agentico\n",
    "    rag_system = AgenticRAG()\n",
    "    \n",
    "    # Carica un documento di esempio\n",
    "    example_docs = []\n",
    "    directory = \"anonymized\" if os.path.exists(\"anonymized\") else \"documents\"\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        files = os.listdir(directory)[:2]  # Prendi solo 2 file per test\n",
    "        for file in files:\n",
    "            file_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                    example_docs.append((file, content))\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nel leggere {file}: {e}\")\n",
    "    \n",
    "    if not example_docs:\n",
    "        # Usa un documento di esempio fittizio\n",
    "        example_docs = [(\"example.txt\", \"\"\"\n",
    "            Oggetto: Richiesta preventivo per fornitura materiale informatico\n",
    "            \n",
    "            Gentile fornitore,\n",
    "            \n",
    "            Con la presente siamo a richiedere un preventivo per l'acquisto del seguente materiale:\n",
    "            \n",
    "            - 10 laptop modello XYZ\n",
    "            - 10 monitor 27\"\n",
    "            - 10 docking station universali\n",
    "            \n",
    "            Restiamo in attesa di un vostro riscontro.\n",
    "            \n",
    "            Cordiali saluti,\n",
    "            Ufficio Acquisti\n",
    "        \"\"\")]\n",
    "    \n",
    "    # Processa ogni documento\n",
    "    results = []\n",
    "    \n",
    "    for file_name, content in example_docs:\n",
    "        print(f\"\\n\\n{'='*50}\")\n",
    "        print(f\"Analisi documento: {file_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Process with the agentic system\n",
    "        agent_state = rag_system.process_document(\n",
    "            document_content=content,\n",
    "            metadata={\"file_name\": file_name}\n",
    "        )\n",
    "        \n",
    "        # Genera e stampa il report\n",
    "        report = rag_system.format_agent_report(agent_state)\n",
    "        print(report)\n",
    "        \n",
    "        # Salva lo stato dell'agente (opzionale)\n",
    "        output_dir = \"agent_outputs\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f\"agent_state_{file_name.replace('.', '_')}.json\")\n",
    "        rag_system.save_agent_state(agent_state, output_path)\n",
    "        \n",
    "        # Salva risultati per confronto\n",
    "        results.append({\n",
    "            \"file\": file_name,\n",
    "            \"classification\": agent_state.final_decision,\n",
    "            \"confidence\": agent_state.confidence,\n",
    "            \"reasoning_steps\": len(agent_state.reasoning_steps)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Esegui il test (decommentare per eseguire)\n",
    "# test_results = test_agentic_rag_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820774f",
   "metadata": {},
   "source": [
    "## Test del Sistema Agentico RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import time\n",
    "\n",
    "# Flag per l'esecuzione del test\n",
    "run_agentic_test = False  # Cambiare in True per eseguire il test\n",
    "\n",
    "if run_agentic_test:\n",
    "    # Inizializza il sistema RAG agentico\n",
    "    print(\"Inizializzazione del sistema RAG agentico...\")\n",
    "    rag_system = AgenticRAG()\n",
    "    \n",
    "    # Directory contenente i documenti da analizzare\n",
    "    directory = \"anonymized\" if os.path.exists(\"anonymized\") else \"documents\"\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} non trovata! Creazione di un documento di esempio...\")\n",
    "        os.makedirs(\"documents\", exist_ok=True)\n",
    "        with open(\"documents/example.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\"\"\n",
    "            Oggetto: Richiesta preventivo per fornitura materiale informatico\n",
    "            \n",
    "            Gentile fornitore,\n",
    "            \n",
    "            Con la presente siamo a richiedere un preventivo per l'acquisto del seguente materiale:\n",
    "            \n",
    "            - 10 laptop modello XYZ\n",
    "            - 10 monitor 27\"\n",
    "            - 10 docking station universali\n",
    "            \n",
    "            Restiamo in attesa di un vostro riscontro.\n",
    "            \n",
    "            Cordiali saluti,\n",
    "            Ufficio Acquisti\n",
    "            \"\"\")\n",
    "        \n",
    "        directory = \"documents\"\n",
    "\n",
    "    # Carica i documenti di test (limita a 3 per brevità)\n",
    "    files = os.listdir(directory)[:3]\n",
    "    \n",
    "    # Prepara strutture dati per i risultati\n",
    "    results = []\n",
    "    agent_states = []\n",
    "    \n",
    "    # Analizza ogni documento\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        print(f\"\\nAnalisi del documento: {file}\")\n",
    "        \n",
    "        try:\n",
    "            # Leggi contenuto del documento\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Registra il tempo di inizio\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Processa con il sistema agentico\n",
    "            agent_state = rag_system.process_document(\n",
    "                document_content=content,\n",
    "                metadata={\"file_name\": file, \"file_path\": file_path}\n",
    "            )\n",
    "            \n",
    "            # Calcola tempo di esecuzione\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Salva risultati\n",
    "            agent_states.append(agent_state)\n",
    "            results.append({\n",
    "                \"file\": file,\n",
    "                \"classification\": agent_state.final_decision,\n",
    "                \"confidence\": agent_state.confidence,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"steps\": len(agent_state.reasoning_steps),\n",
    "                \"status\": agent_state.status,\n",
    "                \"num_chunks\": len(agent_state.relevant_chunks)\n",
    "            })\n",
    "            \n",
    "            # Visualizza report del documento corrente\n",
    "            display(Markdown(rag_system.format_agent_report(agent_state)))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nell'analisi di {file}: {e}\")\n",
    "    \n",
    "    # Crea dataframe con risultati\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualizza tabella risultati\n",
    "    print(\"\\n=== RIEPILOGO RISULTATI ===\")\n",
    "    display(df_results)\n",
    "    \n",
    "    # Visualizza grafici dei risultati\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Grafico classificazioni\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if not df_results.empty:\n",
    "        classification_counts = df_results['classification'].value_counts()\n",
    "        classification_counts.plot.bar(color='skyblue')\n",
    "        plt.title('Classificazioni Documenti')\n",
    "        plt.xlabel('Tipo Documento')\n",
    "        plt.ylabel('Numero Documenti')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    # Grafico confidenza\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if not df_results.empty:\n",
    "        plt.bar(df_results['file'], df_results['confidence'], color='lightgreen')\n",
    "        plt.title('Confidenza delle Classificazioni')\n",
    "        plt.xlabel('Documento')\n",
    "        plt.ylabel('Confidenza')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizza dettagli performance\n",
    "    if not df_results.empty:\n",
    "        print(\"\\n=== STATISTICHE PERFORMANCE ===\")\n",
    "        print(f\"Tempo medio di esecuzione: {df_results['execution_time'].mean():.2f} secondi\")\n",
    "        print(f\"Numero medio di step di ragionamento: {df_results['steps'].mean():.1f}\")\n",
    "        print(f\"Numero medio di chunk rilevanti: {df_results['num_chunks'].mean():.1f}\")\n",
    "else:\n",
    "    print(\"Test non eseguito. Per eseguire il test, imposta 'run_agentic_test = True'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7422c",
   "metadata": {},
   "source": [
    "## Confronto tra RAG Standard e RAG Agentico\n",
    "\n",
    "### RAG Standard vs RAG Agentico\n",
    "\n",
    "Il **RAG standard** (Retrieval-Augmented Generation) arricchisce i prompt con informazioni rilevanti recuperate da una knowledge base. Funziona in questo modo:\n",
    "\n",
    "1. Si genera l'embedding del documento\n",
    "2. Si recuperano chunks simili dalla knowledge base\n",
    "3. Si inseriscono i chunks nel prompt\n",
    "4. Si ottiene una risposta che considera sia il documento che le informazioni aggiuntive\n",
    "\n",
    "Il **RAG agentico** invece estende questo approccio aggiungendo capacità di ragionamento autonomo, pianificazione e decisione. I componenti principali sono:\n",
    "\n",
    "1. **Sistema di ragionamento multi-step**: l'agente analizza il documento in più fasi, registrando pensieri e osservazioni\n",
    "2. **Tracciamento dello stato**: l'agente mantiene uno stato interno che evolve durante l'analisi\n",
    "3. **Pianificazione e decisioni**: l'agente decide autonomamente cosa fare con l'informazione\n",
    "4. **Auto-valutazione**: l'agente valuta la propria confidenza nelle decisioni prese\n",
    "5. **Memoria**: l'agente tiene traccia del reasoning e delle decisioni precedenti\n",
    "\n",
    "### Vantaggi del RAG Agentico\n",
    "\n",
    "- **Trasparenza**: l'intero processo decisionale è documentato e ispezionabile\n",
    "- **Confidenza quantificata**: ogni decisione è accompagnata da una misura di confidenza\n",
    "- **Flessibilità**: può adattarsi a diversi tipi di documenti e casi d'uso\n",
    "- **Decision-making avanzato**: ragiona sulle informazioni invece di limitarsi a classificare\n",
    "- **Persistenza dello stato**: il processo può essere salvato, analizzato e ripreso\n",
    "\n",
    "### Limitazioni del RAG Agentico\n",
    "\n",
    "- **Complessità**: richiede più chiamate API e logica applicativa\n",
    "- **Costo computazionale**: richiede più risorse rispetto al RAG standard\n",
    "- **Latenza**: il processo multi-step può richiedere più tempo\n",
    "- **Overhead di implementazione**: richiede più codice e manutenzione\n",
    "\n",
    "### Casi d'uso ideali per il RAG Agentico\n",
    "\n",
    "- Analisi di documenti complessi\n",
    "- Processi decisionali che richiedono più passaggi\n",
    "- Situazioni in cui è essenziale la trasparenza del ragionamento\n",
    "- Attività che richiedono un alto livello di confidenza nelle decisioni\n",
    "- Workflow che potrebbero richiedere azioni differenti in base al contenuto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80ee87",
   "metadata": {},
   "source": [
    "## Integrazione di DOCUMENT_TYPE_PROMPT con il Sistema Agentico RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418829a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Carica variabili d'ambiente\n",
    "load_dotenv()\n",
    "\n",
    "# DOCUMENT_TYPE_PROMPT originale\n",
    "DOCUMENT_TYPE_PROMPT = \"\"\"\n",
    "Ti fornirò un documento aziendale.\n",
    " \n",
    "Il tuo compito è determinare il tipo di documento, scegliendo tra i seguenti: \n",
    "- Mail\n",
    "- Nota di credito\n",
    "- Ordine di acquisto\n",
    "- Contratto\n",
    "- Altro\n",
    " \n",
    "Devi basarti solo sul contenuto del documento fornito.\n",
    " \n",
    "Ora incollerò il contenuto del documento tra tripli apici. Rispondi semplicemente con il tipo, nulla di più.\n",
    " \n",
    "Documento:\n",
    "'''\n",
    "{documento}\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "# Versione agentica del DOCUMENT_TYPE_PROMPT\n",
    "AGENTIC_DOCUMENT_TYPE_PROMPT = \"\"\"\n",
    "Ti fornirò un documento aziendale.\n",
    "\n",
    "Il tuo compito è determinare il tipo di documento, scegliendo tra i seguenti: \n",
    "- Mail\n",
    "- Nota di credito\n",
    "- Ordine di acquisto\n",
    "- Contratto\n",
    "- Altro\n",
    "\n",
    "Devi utilizzare un processo di ragionamento strutturato:\n",
    "1. Analizza attentamente il contenuto del documento\n",
    "2. Considera le informazioni rilevanti dalla knowledge base\n",
    "3. Identifica pattern e caratteristiche chiave per la classificazione\n",
    "4. Fornisci un ragionamento dettagliato per la tua classificazione\n",
    "5. Esprimi un livello di confidenza nella tua decisione (da 0 a 1)\n",
    "\n",
    "Rispondi in formato JSON con:\n",
    "- thoughts: il tuo ragionamento dettagliato\n",
    "- classification: il tipo di documento\n",
    "- confidence: un valore numerico tra 0 e 1\n",
    "- evidence: elementi o frasi specifiche che hanno guidato la tua decisione\n",
    "\n",
    "Ecco informazioni rilevanti dalla knowledge base:\n",
    "\n",
    "{context}\n",
    "\n",
    "Ora, analizza il documento tra tripli apici:\n",
    "'''\n",
    "{documento}\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "class IntegratedAgenticDocumentClassifier:\n",
    "    \"\"\"Integratore tra il DOCUMENT_TYPE_PROMPT originale e il sistema RAG agentico\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_base_path=\"chunks_with_embeddings.pkl\"):\n",
    "        \"\"\"Inizializza il classificatore con la knowledge base\"\"\"\n",
    "        self.rag_system = AgenticRAG(knowledge_base_path)\n",
    "        \n",
    "        # Configurazione API\n",
    "        self.api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        self.endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "        self.api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "        self.deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT')\n",
    "        \n",
    "        self.client = openai.AzureOpenAI(\n",
    "            api_key=self.api_key,\n",
    "            azure_endpoint=self.endpoint,\n",
    "            api_version=self.api_version\n",
    "        )\n",
    "    \n",
    "    def _enhance_prompt_with_context(self, document_text):\n",
    "        \"\"\"Arricchisce il prompt con contesto rilevante\"\"\"\n",
    "        # Crea documento e trova chunks rilevanti\n",
    "        doc = Document(\n",
    "            id=f\"doc_{datetime.now().timestamp()}\",\n",
    "            content=document_text\n",
    "        )\n",
    "        \n",
    "        # Genera embedding\n",
    "        doc.embedding = self.rag_system.create_embedding(doc.content[:1500])\n",
    "        if doc.embedding is None:\n",
    "            return DOCUMENT_TYPE_PROMPT.format(documento=document_text), []\n",
    "        \n",
    "        # Recupera chunk rilevanti\n",
    "        relevant_chunks = self.rag_system.retrieve_relevant_chunks(doc, top_k=3)\n",
    "        \n",
    "        # Se non ci sono chunk rilevanti, usa il prompt standard\n",
    "        if not relevant_chunks:\n",
    "            return DOCUMENT_TYPE_PROMPT.format(documento=document_text), []\n",
    "        \n",
    "        # Componi il contesto\n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(relevant_chunks):\n",
    "            context_parts.append(\n",
    "                f\"[FONTE {i+1}] (Rilevanza: {chunk.relevance_score:.4f})\\n{chunk.content}\"\n",
    "            )\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Crea prompt arricchito\n",
    "        enhanced_prompt = AGENTIC_DOCUMENT_TYPE_PROMPT.format(\n",
    "            context=context,\n",
    "            documento=document_text\n",
    "        )\n",
    "        \n",
    "        return enhanced_prompt, relevant_chunks\n",
    "    \n",
    "    def classify_standard(self, document_text):\n",
    "        \"\"\"Classifica usando il DOCUMENT_TYPE_PROMPT originale\"\"\"\n",
    "        prompt = DOCUMENT_TYPE_PROMPT.format(documento=document_text)\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Sei un analizzatore di documenti aziendali.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    \n",
    "    def classify_agentic(self, document_text):\n",
    "        \"\"\"Classifica usando l'approccio agentico\"\"\"\n",
    "        # Processa il documento con il sistema agentico\n",
    "        agent_state = self.rag_system.process_document(document_text)\n",
    "        \n",
    "        return {\n",
    "            \"classification\": agent_state.final_decision,\n",
    "            \"confidence\": agent_state.confidence,\n",
    "            \"reasoning\": [step.thought for step in agent_state.reasoning_steps],\n",
    "            \"num_chunks\": len(agent_state.relevant_chunks)\n",
    "        }\n",
    "    \n",
    "    def classify_hybrid(self, document_text):\n",
    "        \"\"\"Approccio ibrido che usa il prompt arricchito ma non l'intero sistema agentico\"\"\"\n",
    "        enhanced_prompt, relevant_chunks = self._enhance_prompt_with_context(document_text)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.deployment_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Sei un analizzatore di documenti aziendali.\"},\n",
    "                    {\"role\": \"user\", \"content\": enhanced_prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            return {\n",
    "                \"classification\": result.get(\"classification\", \"Altro\"),\n",
    "                \"confidence\": result.get(\"confidence\", 0.5),\n",
    "                \"thoughts\": result.get(\"thoughts\", \"\"),\n",
    "                \"evidence\": result.get(\"evidence\", \"\"),\n",
    "                \"num_chunks\": len(relevant_chunks)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Errore nella classificazione ibrida: {e}\")\n",
    "            # Fallback a classificazione standard\n",
    "            return {\n",
    "                \"classification\": self.classify_standard(document_text),\n",
    "                \"confidence\": 0.5,\n",
    "                \"thoughts\": \"Errore nell'elaborazione agentica, usando classificazione standard.\",\n",
    "                \"evidence\": \"\",\n",
    "                \"num_chunks\": len(relevant_chunks)\n",
    "            }\n",
    "    \n",
    "    def compare_approaches(self, document_text):\n",
    "        \"\"\"Confronta i tre approcci diversi\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        print(\"Confronto dei tre approcci di classificazione...\")\n",
    "        \n",
    "        # Standard (originale)\n",
    "        start = time.time()\n",
    "        standard_result = self.classify_standard(document_text)\n",
    "        standard_time = time.time() - start\n",
    "        results[\"standard\"] = {\n",
    "            \"classification\": standard_result,\n",
    "            \"time\": standard_time,\n",
    "            \"approach\": \"Standard (DOCUMENT_TYPE_PROMPT originale)\"\n",
    "        }\n",
    "        print(f\"Classificazione standard: {standard_result} ({standard_time:.2f}s)\")\n",
    "        \n",
    "        # Hybrid (prompt arricchito)\n",
    "        start = time.time()\n",
    "        hybrid_result = self.classify_hybrid(document_text)\n",
    "        hybrid_time = time.time() - start\n",
    "        results[\"hybrid\"] = {\n",
    "            **hybrid_result,\n",
    "            \"time\": hybrid_time,\n",
    "            \"approach\": \"Hybrid (DOCUMENT_TYPE_PROMPT con RAG)\"\n",
    "        }\n",
    "        print(f\"Classificazione ibrida: {hybrid_result['classification']} (confidenza: {hybrid_result['confidence']:.2f}, {hybrid_time:.2f}s)\")\n",
    "        \n",
    "        # Agentic (completo)\n",
    "        start = time.time()\n",
    "        agentic_result = self.classify_agentic(document_text)\n",
    "        agentic_time = time.time() - start\n",
    "        results[\"agentic\"] = {\n",
    "            **agentic_result,\n",
    "            \"time\": agentic_time,\n",
    "            \"approach\": \"Agentic (Sistema RAG completo)\"\n",
    "        }\n",
    "        print(f\"Classificazione agentica: {agentic_result['classification']} (confidenza: {agentic_result['confidence']:.2f}, {agentic_time:.2f}s)\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Funzione per dimostrare l'integrazione\n",
    "def test_integrated_approach(run_demo=False):\n",
    "    if not run_demo:\n",
    "        print(\"Demo non eseguita. Imposta run_demo=True per eseguire.\")\n",
    "        return\n",
    "    \n",
    "    # Inizializza il classificatore integrato\n",
    "    classifier = IntegratedAgenticDocumentClassifier()\n",
    "    \n",
    "    # Carica un documento di test\n",
    "    document_text = \"\"\n",
    "    directory = \"anonymized\" if os.path.exists(\"anonymized\") else \"documents\"\n",
    "    \n",
    "    if os.path.exists(directory) and os.listdir(directory):\n",
    "        # Prendi il primo file\n",
    "        file_path = os.path.join(directory, os.listdir(directory)[0])\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            document_text = f.read()\n",
    "    else:\n",
    "        # Usa un documento di esempio\n",
    "        document_text = \"\"\"\n",
    "        Oggetto: Richiesta preventivo per fornitura materiale informatico\n",
    "        \n",
    "        Gentile fornitore,\n",
    "        \n",
    "        Con la presente siamo a richiedere un preventivo per l'acquisto del seguente materiale:\n",
    "        \n",
    "        - 10 laptop modello XYZ\n",
    "        - 10 monitor 27\"\n",
    "        - 10 docking station universali\n",
    "        \n",
    "        Restiamo in attesa di un vostro riscontro entro il 15/07/2025.\n",
    "        \n",
    "        Cordiali saluti,\n",
    "        Ufficio Acquisti\n",
    "        \"\"\"\n",
    "    \n",
    "    # Confronta i tre approcci\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CONFRONTO TRA I TRE APPROCCI\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = classifier.compare_approaches(document_text)\n",
    "    \n",
    "    # Visualizza riepilogo\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"RIEPILOGO RISULTATI\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    print(f\"Standard: {results['standard']['classification']} ({results['standard']['time']:.2f}s)\")\n",
    "    print(f\"Hybrid: {results['hybrid']['classification']} (conf: {results['hybrid']['confidence']:.2f}, {results['hybrid']['time']:.2f}s)\")\n",
    "    print(f\"Agentic: {results['agentic']['classification']} (conf: {results['agentic']['confidence']:.2f}, {results['agentic']['time']:.2f}s)\")\n",
    "    \n",
    "    # Visualizza dettagli del ragionamento agentico\n",
    "    if results['agentic']['reasoning']:\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"RAGIONAMENTO AGENTICO\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for i, reasoning in enumerate(results['agentic']['reasoning']):\n",
    "            print(f\"Step {i+1}: {reasoning[:150]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Esegui il test (decommentare per eseguire)\n",
    "# demo_results = test_integrated_approach(run_demo=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
